<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>LOAM_Livox论文笔记</title>
    <link href="/2025/08/20/25_08_20LOAM-Livox%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/08/20/25_08_20LOAM-Livox%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p><img src="/%5Cimg%5CLOAM_Livox1.png" alt="LOAM_Livox"></p><h1 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h1><ul><li>在非常有限的FoV中进行特征提取和选择</li><li>鲁棒的离群点剔除：激光雷达的扫描线束和扫描轨迹分布不均匀</li><li>移动对象过滤</li><li>运动失真补偿</li></ul><h1 id="POINTS-SELECTION-AND-FEATURE-EXTRACTION"><a href="#POINTS-SELECTION-AND-FEATURE-EXTRACTION" class="headerlink" title="POINTS SELECTION AND FEATURE EXTRACTION"></a>POINTS SELECTION AND FEATURE EXTRACTION</h1><p>受激光雷达在真实物理世界的限制，需要在原始点云信息中提取出好的特征点</p><ol><li>分类<br> 按照特定指标选择点<ol><li>对于每个点计算下列指标</li></ol><ul><li>Depth D：点到雷达的距离</li><li>laser deflection angle φ ：X轴与激光线夹角</li><li>intensity I ：反射强度，R是物体反射率，由雷达测量</li><li>incident angle θ ：入射角，激光层与测量点周围的局部平面之间的角度<img src="/%5Cimg%5CLOAM_Livox4.png" alt="入射角示意图"></li></ul><ol start="2"><li>计算公式<br> <img src="/%5Cimg%5CLOAM_Livox2.png" alt="1"><br> <img src="/%5Cimg%5CLOAM_Livox3.png" alt="2"></li><li>舍弃原则</li></ol><ul><li>靠近雷达视野边缘。此时扫描轨迹不可靠</li><li>intensity太大或太小。都会降低测量的准确率</li><li>入射角接近π或0的点。会产生光斑导致测量的是一片范围的均值而不是某一特定点</li><li>被障碍物遮挡的点。会被误判为边缘点</li></ul></li><li>特征提取<ul><li>计算局部平滑度（应该是曲率）来提取边缘点和平面点</li><li>引入反射率作为判断依据，如果由一点和周围点的反射率区别过大，被视为（材料）边缘点</li></ul></li></ol><h1 id="ITERATIVE-POSE-OPTIMIZATION-迭代姿态优化"><a href="#ITERATIVE-POSE-OPTIMIZATION-迭代姿态优化" class="headerlink" title="ITERATIVE POSE OPTIMIZATION  迭代姿态优化"></a>ITERATIVE POSE OPTIMIZATION  迭代姿态优化</h1><p>受雷达非重复扫描的影响，不能利用重复点的匹配来获得位姿</p><ol><li>边对边的残差计算<ol><li>在当前帧中选取一点Pl，再从全局地图中选取距离Pl最近的5个点：先将Pl映射到地图系，在从地图系的KDtree里面选取5个点</li><li>以当前帧的最后一点Pw的雷达姿态为准，计算当前帧到地图的转换矩阵来实现投影</li><li>确保选取的5个Pi和点Pw在同一条直线上    </li><li>残差计算<br> <img src="/%5Cimg%5CLOAM_Livox5.png" alt="3和4计算思路"></li></ol></li><li>面对面的残差计算<br> 和边对边的残差计算思路相同，就是计算是否是同一平面上由不同<br> <img src="/%5Cimg%5CLOAM_Livox6.png" alt="计算思路"></li><li>帧内运动补偿<ol><li>分段处理：同一帧分成几部分，独立进行处理。相当于缩短运动时间，减轻运动带来的影响，分开处理还可以并行计算，缩短计算时间</li><li>线性插值：假设在一帧中运动是均匀的，可以利用最后一个点的姿态变化去估计之前点的转换矩阵<img src="/%5Cimg%5CLOAM_Livox7.png" alt="线性插值计算细节"></li></ol></li><li>离群点剔除和移动对象过滤<br> 避免移动物体对扫描精度产生影响<br> 思路：在每次迭代重新计算所有特征点对应的残差，先做迭代优化残差，最后在优化后结果中去除前20%最大的点（边缘点和平面点分开进行处理）</li></ol><h1 id="个人评价"><a href="#个人评价" class="headerlink" title="个人评价"></a>个人评价</h1><p>亮点：</p><ul><li>特征提取时引入反射强度作为评价依据</li><li>迭代姿态优化可以解决雷达扫描不重复的问题</li><li>将优化后残差较大的点作为运动点</li></ul>]]></content>
    
    
    <categories>
      
      <category>SLAM</category>
      
      <category>LOAM</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>SSD,YOLO物体检测算法</title>
    <link href="/2025/08/19/25_08_19SSD%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/"/>
    <url>/2025/08/19/25_08_19SSD%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h1><h2 id="锚框选择"><a href="#锚框选择" class="headerlink" title="锚框选择"></a>锚框选择</h2><p>以每个像素为中心，生成多个锚框</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>多次卷积得到不同大小的图（关注范围不同），每次都有一遍锚框选择<br><img src="/%5Cimg%5CSSD.png" alt="SSD模型结构"></p><h1 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h1><h2 id="锚框选择-1"><a href="#锚框选择-1" class="headerlink" title="锚框选择"></a>锚框选择</h2><p>将图片均匀分为S*S个锚框，每个锚框去预测多个种类边缘框（避免单个锚框框住多个种类目标）</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>物体检测</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>RCNN物体检测算法</title>
    <link href="/2025/08/19/25_08_19RCNN%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/"/>
    <url>/2025/08/19/25_08_19RCNN%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h1><p><img src="/img/RCNN1.png" alt="示意图"></p><ul><li>启发式搜索来选择锚框</li><li>通过CNN提取特征</li><li>基于锚框的分类和回归<ul><li>分类: SVM</li><li>回归: 线性回归,来获得锚框和目标框的偏移量</li></ul></li></ul><h2 id="兴趣区域（ROI）池化层"><a href="#兴趣区域（ROI）池化层" class="headerlink" title="兴趣区域（ROI）池化层"></a>兴趣区域（ROI）池化层</h2><p>将不同大小的特征图转换为固定大小的特征图</p><ul><li>输入: 特征图, 锚框</li><li>输出: 固定大小的特征图</li><li>过程:<ul><li>对每个锚框, 均匀分割为固定数量的区域</li><li>对每个区域, 取像素最大值</li><li>输出固定大小的特征图<br><img src="/img/RCNN2.png" alt="ROI池化"></li></ul></li></ul><h1 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h1><h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><ul><li>改变了启发式搜索的位置</li><li>引入了CNN的特征提取层，在ROI池化层之前。在原图像中启发式搜索获取锚框，映射到CNN的输出上，再交给CNN。可以先抽取全局的特征再进行ROI池化，而不是先ROI池化再过CNN.</li></ul><h1 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h1><h2 id="改进-相对于-Fast-R-CNN"><a href="#改进-相对于-Fast-R-CNN" class="headerlink" title="改进(相对于 Fast R-CNN)"></a>改进(相对于 Fast R-CNN)</h2><ul><li>将启发式搜索替换为区域建议网络RPN（Region Proposal Network）<ul><li>输入: 图像</li><li>输出: 锚框</li><li>过程:<ul><li>使用CNN提取特征图</li><li>生成锚框</li><li>预测每个锚框的分类和回归偏移量</li><li>非极大值抑制(NMS)<ul><li>输入: 锚框, 分类分数</li><li>输出: 保留的锚框</li><li>过程:<ul><li>对每个类别, 按分类分数排序</li><li>从高到低遍历, 保留当前锚框, 并移除与当前锚框重叠度高的锚框</li></ul></li></ul></li></ul></li></ul></li></ul><p><img src="/img/RPN.png" alt="RPN"></p><h1 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h1><h2 id="改进-相对于-Faster-R-CNN"><a href="#改进-相对于-Faster-R-CNN" class="headerlink" title="改进(相对于 Faster R-CNN)"></a>改进(相对于 Faster R-CNN)</h2><ul><li>引入了掩码预测层FCN( Fully Convolutional Network)<ul><li>作用：利用像素级的标号</li><li>输入: 锚框</li><li>输出: 掩码</li><li>过程:<ul><li>对每个锚框, 预测一个二进制掩码</li><li>对每个像素, 预测它是否属于物体</li></ul></li></ul></li><li>ROI池化层被替换成ROI align层<ul><li>作用：更准确的提取特征</li><li>输入: 特征图, 锚框</li><li>输出: 固定大小的特征图</li><li>过程:<ul><li>对每个锚框, 均匀分割为固定数量的区域（每个区域的大小相同，交叉的像素点会被按权重拆开）</li><li>对每个区域, 取像素最大值</li><li>输出固定大小的特征图<br><img src="/img/MaskRCNN.png" alt="Mask R-CNN"></li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>物体检测</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ResNet网络</title>
    <link href="/2025/08/18/25_08_18ResNet%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/08/18/25_08_18ResNet%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>问题：随着网络层数的增加，模型的性能会出现饱和甚至下降,增加更多的层反而不能增加精度<br>想法：<br>使得每一层都包含之前层的信息，从而避免信息丢失，即使深度加深，可以保证精度不会下降太多</p><h2 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h2><p>将本次输入加到本次输出中<br><img src="/img/%E6%AE%8B%E5%B7%AE%E5%9D%97.png" alt="残差块"></p><h2 id="ResNet块"><a href="#ResNet块" class="headerlink" title="ResNet块"></a>ResNet块</h2><p><img src="/img/ResNet%E5%9D%97.png" alt="ResNet块"><br>残差块可以进行替换组合</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>批量归一化</title>
    <link href="/2025/08/18/25_08_18%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <url>/2025/08/18/25_08_18%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>底层参数的变化会导致顶层的较大变化，导致训练效果不好</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>批量归一化（Batch Normalization），对每一层的输入进行归一<br>化处理，使得每一层的输入分布保持稳定。</p><ul><li>固定每个小批量的均值和方差</li><li>对每个小批量的输入进行归一化处理</li><li>引入可学习的缩放参数和偏移参数，用于调整归一化后的输出</li></ul><h2 id="批量归一化层"><a href="#批量归一化层" class="headerlink" title="批量归一化层"></a>批量归一化层</h2><ul><li>可学习的缩放参数和偏移参数</li><li>作用位置：全连接层和卷积层的输出，激活函数之前</li><li>作用：归一化输入，加速训练，正则化模型</li></ul><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs css">import torch <br><span class="hljs-selector-tag">from</span> torch import nn<br><span class="hljs-selector-tag">from</span> d2l import torch as d2l<br><br>def batch_norm(<span class="hljs-attribute">X</span>,gamma,beta,moving_mean,moving_var,eps,momentum):<br>    if not torch.<span class="hljs-built_in">is_grad_enabled</span>():<br>        X_hat = (X - moving_mean) / torch.<span class="hljs-built_in">sqrt</span>(moving_var + eps)<br>    else:<br>        assert <span class="hljs-built_in">len</span>(X.shape) in (<span class="hljs-number">2</span>, <span class="hljs-number">4</span>), <span class="hljs-string">&quot;Batch normalization only supports 2D or 4D inputs&quot;</span><br>        if <span class="hljs-built_in">len</span>(X.shape) == <span class="hljs-number">2</span>:<br>            mean = X.<span class="hljs-built_in">mean</span>(dim=<span class="hljs-number">0</span>)<br>            var = ((X - mean) ** <span class="hljs-number">2</span>).<span class="hljs-built_in">mean</span>(dim=<span class="hljs-number">0</span>)<br>        else:<br>            mean = X.<span class="hljs-built_in">mean</span>(dim=(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), keepdim=True)<br>            var = ((X - mean) ** <span class="hljs-number">2</span>).<span class="hljs-built_in">mean</span>(dim=(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), keepdim=True)<br><br>        X_hat = (X - mean) / torch.<span class="hljs-built_in">sqrt</span>(var + eps)<br>        moving_mean = momentum * moving_mean + (<span class="hljs-number">1.0</span> - momentum) * mean# 移动平均<br><br>        moving_var = momentum * moving_var + (<span class="hljs-number">1.0</span> - momentum) * var  # 移动平均<br><br>    Y = gamma * X_hat + beta<br>    return Y, moving_mean, moving_var<br><br>class <span class="hljs-built_in">BatchNorm</span>(nn.Module):<br>    def <span class="hljs-built_in">__init__</span>(self, num_features, num_dims):<br>        <span class="hljs-built_in">super</span>().<span class="hljs-built_in">__init__</span>()<br>        if num_dims == <span class="hljs-number">2</span>:<br>            shape = (<span class="hljs-number">1</span>, num_features)<br>        else:<br>            shape = (<span class="hljs-number">1</span>, num_features, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        self.gamma = nn.<span class="hljs-built_in">Parameter</span>(torch.<span class="hljs-built_in">ones</span>(shape))<br>        self.beta = nn.<span class="hljs-built_in">Parameter</span>(torch.<span class="hljs-built_in">zeros</span>(shape))<br>        self.moving_mean = torch.<span class="hljs-built_in">zeros</span>(shape)<br>        self.moving_var = torch.<span class="hljs-built_in">ones</span>(shape)<br><br>    def <span class="hljs-built_in">forward</span>(self, X):<br>        if self.moving_mean.device != X.device:<br>            self.moving_mean = self.moving_mean.<span class="hljs-built_in">to</span>(X.device)<br>            self.moving_var = self.moving_var.<span class="hljs-built_in">to</span>(X.device)<br>        Y, self.moving_mean, self.moving_var = <span class="hljs-built_in">batch_norm</span>(<br>            X, self.gamma, self.beta, self.moving_mean, self.moving_var,<br>            eps=<span class="hljs-number">1</span>e-<span class="hljs-number">5</span>, momentum=<span class="hljs-number">0.9</span>)<br>        return Y<br><br>if __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    # 测试BatchNorm<br>    # X = torch.<span class="hljs-built_in">randn</span>(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)  # <span class="hljs-number">4</span>个样本，<span class="hljs-number">3</span>个通道，<span class="hljs-number">2</span>x2的特征图<br>    # batch_norm_layer = <span class="hljs-built_in">BatchNorm</span>(num_features=<span class="hljs-number">3</span>, num_dims=<span class="hljs-number">4</span>)<br>    # Y = <span class="hljs-built_in">batch_norm_layer</span>(X)<br>    # <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, Y.shape)  # 应该是 (<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><br><br>    #应用于LeNet<br>    net = nn.<span class="hljs-built_in">Sequential</span>(<br>        nn.<span class="hljs-built_in">Conv2d</span>(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>),<br>        <span class="hljs-built_in">BatchNorm</span>(<span class="hljs-number">6</span>, num_dims=<span class="hljs-number">4</span>),<br>        nn.<span class="hljs-built_in">Sigmoid</span>(),<br>        nn.<span class="hljs-built_in">MaxPool2d</span>(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>        nn.<span class="hljs-built_in">Conv2d</span>(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>),<br>        <span class="hljs-built_in">BatchNorm</span>(<span class="hljs-number">16</span>, num_dims=<span class="hljs-number">4</span>),<br>        nn.<span class="hljs-built_in">Sigmoid</span>(),<br>        nn.<span class="hljs-built_in">MaxPool2d</span>(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>        nn.<span class="hljs-built_in">Flatten</span>(),<br>        nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">16</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">120</span>),<br>        <span class="hljs-built_in">BatchNorm</span>(<span class="hljs-number">120</span>, num_dims=<span class="hljs-number">2</span>),<br>        nn.<span class="hljs-built_in">Sigmoid</span>(),<br>        nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>),<br>        <span class="hljs-built_in">BatchNorm</span>(<span class="hljs-number">84</span>, num_dims=<span class="hljs-number">2</span>),<br>        nn.<span class="hljs-built_in">Sigmoid</span>(),<br>        nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br>    )<br><br>    lr, num_epochs = <span class="hljs-number">1.0</span>, <span class="hljs-number">10</span><br>    batch_size = <span class="hljs-number">256</span><br>    train_iter, test_iter = d2l.<span class="hljs-built_in">load_data_fashion_mnist</span>(batch_size)<br>    d2l.<span class="hljs-built_in">train_ch6</span>(net, train_iter, test_iter, num_epochs, lr, d2l.<span class="hljs-built_in">try_gpu</span>())<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>GoogleNet网络</title>
    <link href="/2025/08/18/25_08_18GoogleNet%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/08/18/25_08_18GoogleNet%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h1><h2 id="Inception模块"><a href="#Inception模块" class="headerlink" title="Inception模块"></a>Inception模块</h2><p>多个路径从不同·层面抽取信息，然后在输出通道合并，即使不同路径的输出通道数不同，也可以通过1*1卷积层来进行通道数的匹配。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="/img/GoogleNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png" alt="GoogleNet网络结构"></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>NiN网络</title>
    <link href="/2025/08/18/25_08_18NiN%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/08/18/25_08_18NiN%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>问题：对于全连接层，在卷积层之后的第一个全连接层的输入通道数是非常大的，这个时候如果使用全连接层，会导致参数数量非常多，从而导致模型的过拟合。</p><h2 id="NiN网络"><a href="#NiN网络" class="headerlink" title="NiN网络"></a>NiN网络</h2><h3 id="NiN块"><a href="#NiN块" class="headerlink" title="NiN块"></a>NiN块</h3><ul><li>一个卷积层后跟两个全连接层</li><li>全连接层：1*1卷积层</li></ul><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><ol><li>无全连接层</li><li>交替使用NiN块和步幅为2的最大池化层（逐步减小高宽和增大通道数）</li><li>最后使用全局平均池化层的到输出（输入通道是类别数）<br><img src="/img/NiN%E6%9E%B6%E6%9E%84.png" alt="NiN架构"></li></ol><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h1 id="定义NiN块"><a href="#定义NiN块" class="headerlink" title="定义NiN块"></a>定义NiN块</h1><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch<br><span class="hljs-keyword">from</span> torch import nn<br><span class="hljs-keyword">from</span> d2l import torch as d2l<br><br>def nin_block(in_channels, out_channels, kernel_size, <span class="hljs-attribute">stride</span>=1, <span class="hljs-attribute">padding</span>=0):<br>    return nn.Sequential(<br>        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),<br>        nn.ReLU(),<br>        nn.Conv2d(out_channels, out_channels, <span class="hljs-attribute">kernel_size</span>=1),  # 1x1卷积<br>        nn.ReLU(),<br>        nn.Conv2d(out_channels, out_channels, <span class="hljs-attribute">kernel_size</span>=1),  # 1x1卷积<br>        nn.ReLU()<br>    )<br><br><br>def nin(<span class="hljs-attribute">num_classes</span>=10):<br>    return nn.Sequential(<br>        nin_block(1, 96, <span class="hljs-attribute">kernel_size</span>=11, <span class="hljs-attribute">stride</span>=4, <span class="hljs-attribute">padding</span>=0),  # 卷积层1<br>        nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),  # 池化层1<br>        nin_block(96, 256, <span class="hljs-attribute">kernel_size</span>=5, <span class="hljs-attribute">stride</span>=1, <span class="hljs-attribute">padding</span>=2),  # 卷积层2<br>        nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),  # 池化层2<br>        nin_block(256, 384, <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=1, <span class="hljs-attribute">padding</span>=1),  # 卷积层3<br>        nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),  # 池化层3<br>        nn.Dropout(0.5),  # 丢弃层<br>        nin_block(384, 10, <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=1, <span class="hljs-attribute">padding</span>=1),  # 卷积层4, 输出层<br>        nn.AdaptiveAvgPool2d((1, 1)),  # 自适应平均池化<br>        nn.Flatten()  # 展平层<br>    )<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    net = nin()<br>    <br>    # 测试网络结构<br>    # X = torch.randn(1, 1, 224, 224)  # 输入图像的形状<br>    # <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    #     X = layer(X)  # 前向传播<br>    #     <span class="hljs-built_in">print</span>(layer.__class__.__name__, <span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br><br>    lr, num_epochs = 0.01, 10<br>    batch_size = 128<br>    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, <span class="hljs-attribute">resize</span>=224)<br>    d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>VGG网络</title>
    <link href="/2025/08/18/25_08_18VGG%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/08/18/25_08_18VGG%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>AlexNet对于LeNet的提升得益于网络的更深更大，网络更深更大的方法：</p><ul><li>更多的全连接层</li><li>更多的卷积层</li><li>将卷积层分成块（VGG）</li></ul><p>和AlexNet的区别：<br>使用尺寸更小的3×3卷积核串联来替代大卷积核11×11,7×7这样的大尺寸卷积核，引入块设计思想，在相同的感受野的情况下，多个串联非线性能力更强，描述能力更强</p><h2 id="VGG思路"><a href="#VGG思路" class="headerlink" title="VGG思路"></a>VGG思路</h2><p>VGG网络的思路是将卷积层分成多个块，每个块包含多个卷积层和一个池化层，最后串联接多个全连接层。<br><img src="/img/VGG%E6%9E%B6%E6%9E%84.png" alt="VGG网络结构"></p><ul><li>VGG-16 13个卷积层与3个全连接层</li><li>VGG-19 16个卷积层与3个全连接层</li></ul><h2 id="VGG实现"><a href="#VGG实现" class="headerlink" title="VGG实现"></a>VGG实现</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs css">import torch<br><span class="hljs-selector-tag">from</span> torch import nn<br><span class="hljs-selector-tag">from</span> d2l import torch as d2l<br><br># 定义VGG块<br>def vgg_block(num_convs, in_channels, out_channels):<br>    layers = []<br>    for _ in <span class="hljs-built_in">range</span>(num_convs):<br>        layers.<span class="hljs-built_in">append</span>(nn.<span class="hljs-built_in">Conv2d</span>(in_channels, out_channels, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>))<br>        layers.<span class="hljs-built_in">append</span>(nn.<span class="hljs-built_in">ReLU</span>())<br>        in_channels = out_channels#更新输入通道数<br>    layers.<span class="hljs-built_in">append</span>(nn.<span class="hljs-built_in">MaxPool2d</span>(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>    return nn.<span class="hljs-built_in">Sequential</span>(*layers)<br><br>def <span class="hljs-built_in">vgg</span>(conv_arch, num_classes=<span class="hljs-number">10</span>):<br>    conv_blocks = []<br>    in_channels = <span class="hljs-number">1</span>  # 输入通道数<br>    for (num_convs, out_channels) in conv_arch:<br>        conv_blocks.<span class="hljs-built_in">append</span>(<span class="hljs-built_in">vgg_block</span>(num_convs, in_channels, out_channels))<br>        in_channels = out_channels<br>    return nn.<span class="hljs-built_in">Sequential</span>(<br>        #卷积单元<br>        *conv_blocks,<br>        #全连接单元<br>        nn.<span class="hljs-built_in">Flatten</span>(),<br>        nn.<span class="hljs-built_in">Linear</span>(in_channels * <span class="hljs-number">7</span> * <span class="hljs-number">7</span>, <span class="hljs-number">4096</span>),  # 假设输入图像大小为<span class="hljs-number">224</span>x224<br>        nn.<span class="hljs-built_in">ReLU</span>(),<br>        nn.<span class="hljs-built_in">Dropout</span>(<span class="hljs-number">0.5</span>),<br>        nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>),<br>        nn.<span class="hljs-built_in">ReLU</span>(),<br>        nn.<span class="hljs-built_in">Dropout</span>(<span class="hljs-number">0.5</span>),<br>        nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">4096</span>, num_classes)<br>    )<br><br>if __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    conv_arch = [(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">512</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">512</span>)]  # VGG架构<br>    net = <span class="hljs-built_in">vgg</span>(conv_arch)<br>    <br>    # 测试网络结构<br>    # X = torch.<span class="hljs-built_in">randn</span>(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)  # 输入图像的形状<br>    # for layer in net:<br>    #     X = <span class="hljs-built_in">layer</span>(X)  # 前向传播<br>    #     <span class="hljs-built_in">print</span>(layer.__class__.__name__, <span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br><br>    batch_size = <span class="hljs-number">128</span><br>    train_iter, test_iter = d2l.<span class="hljs-built_in">load_data_fashion_mnist</span>(batch_size, resize=<span class="hljs-number">224</span>)<br>    lr, num_epochs = <span class="hljs-number">0.01</span>, <span class="hljs-number">10</span><br>    d2l.<span class="hljs-built_in">train_ch6</span>(net, train_iter, test_iter, num_epochs, lr, d2l.<span class="hljs-built_in">try_gpu</span>())<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>AlexNet网络</title>
    <link href="/2025/08/18/25_08_18AlexNet%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/08/18/25_08_18AlexNet%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><h2 id="改进（基于LeNet）"><a href="#改进（基于LeNet）" class="headerlink" title="改进（基于LeNet）"></a>改进（基于LeNet）</h2><ol><li>采用ReLU激活函数（解决了梯度消失问题）</li><li>丢弃法</li><li>MaxPooling</li><li>数据增强</li></ol><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>和LeNet相比，网络结构更复杂，层和核更大<br><img src="/img/Alexnet1.png" alt="网络结构"><br><img src="/img/Alexnet2.png" alt="网络结构-多次卷积"><br><img src="/img/Alexnet3.png" alt="网络结构-全连接层"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch<br><span class="hljs-keyword">from</span> torch import nn<br><span class="hljs-keyword">from</span> d2l import torch as d2l<br><br>net = nn.Sequential(<br>    #卷积单元<br>    nn.Conv2d(1,96,<span class="hljs-attribute">kernel_size</span>=11,stride=4, <span class="hljs-attribute">padding</span>=1), nn.ReLU(),<br>    nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),<br>    nn.Conv2d(96, 256, <span class="hljs-attribute">kernel_size</span>=5, <span class="hljs-attribute">padding</span>=2), nn.ReLU(),<br>    nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),<br>    nn.Conv2d(256, 384, <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">padding</span>=1), nn.ReLU(),<br>    nn.Conv2d(384, 384, <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">padding</span>=1), nn.ReLU(),<br>    nn.Conv2d(384, 256, <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">padding</span>=1), nn.ReLU(),<br>    nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),<br>    #全连接单元<br>    nn.Flatten(),<br>    nn.Linear(6400, 4096), nn.ReLU(),<br>    nn.Dropout(0.5),     #丢弃层<br>    nn.Linear(4096, 4096), nn.ReLU(),   <br>    nn.Dropout(0.5),<br>    nn.Linear(4096, 10)<br>)<br><br><span class="hljs-comment"># X = torch.randn(1, 1, 224, 224)  # 输入图像的形状</span><br><span class="hljs-comment"># for layer in net:</span><br><span class="hljs-comment">#     X = layer(X)  # 前向传播</span><br><span class="hljs-comment">#     print(layer.__class__.__name__, &#x27;output shape:\t&#x27;, X.shape)</span><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    batch_size = 128<br>    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, <span class="hljs-attribute">resize</span>=224)<br>    lr, num_epochs = 0.01, 10<br>    d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>池化层</title>
    <link href="/2025/08/18/25_08_18%E6%B1%A0%E5%8C%96%E5%B1%82/"/>
    <url>/2025/08/18/25_08_18%E6%B1%A0%E5%8C%96%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>卷积对位置信息的敏感度较高，而池化层对位置信息的敏感度较低。<br>池化层的作用是降低卷积层的敏感度，减少计算量，防止过拟合。</p><h2 id="池化方式"><a href="#池化方式" class="headerlink" title="池化方式"></a>池化方式</h2><ol><li><strong>最大池化</strong>: 取池化窗口内的最大值作为输出。</li><li><strong>平均池化</strong>: 取池化窗口内的平均值作为输出。</li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pool2d</span>(<span class="hljs-params">X, pool_size, mode = <span class="hljs-string">&#x27;max&#x27;</span></span>):<br>    p_h, p_w = pool_size<br>    Y = torch.zeros((X.shape[<span class="hljs-number">0</span>] - p_h + <span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>] - p_w + <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">0</span>]):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">1</span>]):<br>            <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;max&#x27;</span>:<br>                Y[i,j] = X[i:i+p_h,j:j+p_w].<span class="hljs-built_in">max</span>()<br>            <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;avg&#x27;</span>:<br>                Y[i,j] = X[i:i+p_h,j:j+p_w].mean()<br>    <span class="hljs-keyword">return</span> Y<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>LOAM论文笔记</title>
    <link href="/2025/08/18/25_08_18LOAM%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/08/18/25_08_18LOAM%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="LOAM-Lidar-Odometry-and-Mapping-in-Real-time-笔记"><a href="#LOAM-Lidar-Odometry-and-Mapping-in-Real-time-笔记" class="headerlink" title="LOAM: Lidar Odometry and Mapping in Real-time 笔记"></a>LOAM: Lidar Odometry and Mapping in Real-time 笔记</h1><h2 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h2><p>实时性的位姿估计定位和建图算法，既有高频率低精度里程计进行运动估计，又有低频率高精度点云匹配来地图建图。</p><h2 id="算法一-LIDAR-ODOMETRY"><a href="#算法一-LIDAR-ODOMETRY" class="headerlink" title="算法一  LIDAR ODOMETRY"></a>算法一  LIDAR ODOMETRY</h2><ol><li><strong>特征点匹配</strong>：通过计算曲率c来判断点是否为角点或面点。在c的计算中引入S来消除距离影响，将每一次扫描分成多个相同的子区域，保证特征点的均匀分布。还要注意排除几种特殊情况。</li><li><strong>寻找对应特征点集合</strong>：<ul><li>先将前一时刻的点云投影到当前时刻。</li><li>利用两个时刻的点云信息来进行角点和面点的匹配，使用最近邻搜索相邻特征点，角点进行点到直线的距离计算，面点进行点到平面的距离计算。</li></ul></li><li><strong>运动动作估计</strong>：<ul><li>通过角点和面点的匹配来计算变换矩阵T。</li><li>知道在一段时间内的变化矩阵T,假设运动是均匀的，可以利用时间进行线性插值，得到当前时刻的变换矩阵T。</li><li>利用变换矩阵T来根据投影量更新当前时刻的点云坐标。</li><li>进而将该特征点的距离公式作为损失函数，利用优化算法来求解变换矩阵T，使得距离趋近于0.</li></ul></li><li><strong>伪代码</strong><br><img src="/img/LOAM%E4%BC%AA%E4%BB%A3%E7%A0%811.png" alt="伪代码"></li></ol><h2 id="算法二-LIDAR-MAPPING"><a href="#算法二-LIDAR-MAPPING" class="headerlink" title="算法二  LIDAR MAPPING"></a>算法二  LIDAR MAPPING</h2>]]></content>
    
    
    <categories>
      
      <category>SLAM</category>
      
      <category>LOAM</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>拉普拉斯变换，带通滤波器变换手稿整理</title>
    <link href="/2025/08/17/25_08_17%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A2%EF%BC%8C%E5%B8%A6%E9%80%9A%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%8F%98%E6%8D%A2%E6%89%8B%E7%A8%BF%E6%95%B4%E7%90%86/"/>
    <url>/2025/08/17/25_08_17%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A2%EF%BC%8C%E5%B8%A6%E9%80%9A%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%8F%98%E6%8D%A2%E6%89%8B%E7%A8%BF%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="拉普拉斯变换，带通滤波器变换手稿整理"><a href="#拉普拉斯变换，带通滤波器变换手稿整理" class="headerlink" title="拉普拉斯变换，带通滤波器变换手稿整理"></a>拉普拉斯变换，带通滤波器变换手稿整理</h1><p>拉普拉斯变换，滤波器<br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A21.jpg" alt="1"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A22.jpg" alt="2"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A23.jpg" alt="3"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A24.jpg" alt="4"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A25.jpg" alt="5"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A26.jpg" alt="6"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A27.jpg" alt="7"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A28.jpg" alt="8"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A29.jpg" alt="9"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A210.jpg" alt="10"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A211.jpg" alt="11"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A212.jpg" alt="12"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A213.jpg" alt="13"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A214.jpg" alt="14"></p>]]></content>
    
    
    <categories>
      
      <category>无人机</category>
      
      <category>姿态解算</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>傅里叶变换手稿整理</title>
    <link href="/2025/08/17/25_08_17%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E6%89%8B%E7%A8%BF%E6%95%B4%E7%90%86/"/>
    <url>/2025/08/17/25_08_17%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E6%89%8B%E7%A8%BF%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="傅里叶变换手稿整理"><a href="#傅里叶变换手稿整理" class="headerlink" title="傅里叶变换手稿整理"></a>傅里叶变换手稿整理</h1><p>傅里叶变换详解<br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(1).jpg" alt="1"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(2).jpg" alt="2"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(3).jpg" alt="3"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(4).jpg" alt="4"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(5).jpg" alt="5"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(6).jpg" alt="6"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(7).jpg" alt="7"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(8).jpg" alt="8"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(9).jpg" alt="9"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(10).jpg" alt="10"></p>]]></content>
    
    
    <categories>
      
      <category>无人机</category>
      
      <category>姿态解算</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>姿态解算手稿整理</title>
    <link href="/2025/08/17/25_08_17%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97%E6%89%8B%E7%A8%BF%E6%95%B4%E7%90%86/"/>
    <url>/2025/08/17/25_08_17%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97%E6%89%8B%E7%A8%BF%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="手稿整理"><a href="#手稿整理" class="headerlink" title="手稿整理"></a>手稿整理</h1><p>从一开始的姿态表示（欧拉角、四元数、旋转矩阵），到滤波用的拉普拉斯变换，再到控制PID。<br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF1.jpg" alt="1"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF2.jpg" alt="2"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF3.jpg" alt="3"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF4.jpg" alt="4"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF5.jpg" alt="5"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF6.jpg" alt="6"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF7.jpg" alt="7"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF8.jpg" alt="8"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF9.jpg" alt="9"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF10.jpg" alt="10"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF11.jpg" alt="11"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF12.jpg" alt="12"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF13.jpg" alt="13"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF14.jpg" alt="14"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF15.jpg" alt="15"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF16.jpg" alt="16"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF17.jpg" alt="17"></p>]]></content>
    
    
    <categories>
      
      <category>无人机</category>
      
      <category>姿态解算</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>姿态解算资料整理</title>
    <link href="/2025/08/17/25_08_17%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/"/>
    <url>/2025/08/17/25_08_17%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="资料整理"><a href="#资料整理" class="headerlink" title="资料整理"></a>资料整理</h1><p><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_01.png" alt="1"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_02.png" alt="2"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_03.png" alt="3"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_04.png" alt="4"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_05.png" alt="5"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_06.png" alt="6"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_07.png" alt="7"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_08.png" alt="8"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_09.png" alt="9"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_10.png" alt="10"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_11.png" alt="11"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_12.png" alt="12"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_13.png" alt="13"></p>]]></content>
    
    
    <categories>
      
      <category>无人机</category>
      
      <category>姿态解算</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>卷积层</title>
    <link href="/2025/08/17/25_08_17%E5%8D%B7%E7%A7%AF%E5%B1%82/"/>
    <url>/2025/08/17/25_08_17%E5%8D%B7%E7%A7%AF%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h1><h2 id="分类原则"><a href="#分类原则" class="headerlink" title="分类原则"></a>分类原则</h2><ul><li>平移不变性：分类器对输入的平移不变性，即输入的图像平移后，分类器的输出不变。</li><li>局部性：分类器关注输入的局部性</li></ul><h2 id="卷积层是一个特殊的全连接层"><a href="#卷积层是一个特殊的全连接层" class="headerlink" title="卷积层是一个特殊的全连接层"></a>卷积层是一个特殊的全连接层</h2><ol><li>设计原理：<br><img src="/img/%E5%8D%B7%E7%A7%AF%E5%B1%821.png" alt="平移不变性"><br><img src="/img/%E5%8D%B7%E7%A7%AF%E5%B1%822.png" alt="局部性"></li><li>对全连接层进行变形：<br><img src="/img/%E5%8D%B7%E7%A7%AF%E5%B1%823.png" alt="变形"></li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p><strong>二维互相关和二维卷积的实现:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">corr2d</span>(<span class="hljs-params">X, K</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span><br>    h, w = K.shape<br>    Y = torch.zeros((X.shape[<span class="hljs-number">0</span>] - h + <span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>] - w + <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">0</span>]):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">1</span>]):<br>            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="hljs-built_in">sum</span>()<br>    <span class="hljs-keyword">return</span> Y<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Conv2D</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;计算二维卷积层&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, kernel_size</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.weight = nn.Parameter(torch.rand(kernel_size))<br>        <span class="hljs-variable language_">self</span>.bias = nn.Parameter(torch.zeros(<span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> corr2d(X, <span class="hljs-variable language_">self</span>.weight) + <span class="hljs-variable language_">self</span>.bias<br><br></code></pre></td></tr></table></figure><p><strong>学习由X生成Y的卷积核:</strong></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">conv2d</span> = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, kernel_size=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), bias=False)<br><br><span class="hljs-attribute">X</span> = X.reshape((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>))<br><span class="hljs-attribute">Y</span> = Y.reshape((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>))<br><br><span class="hljs-attribute">for</span> i in range(<span class="hljs-number">10</span>):<br>    <span class="hljs-attribute">Y_hat</span> = conv2d(X)<br>    <span class="hljs-attribute">l</span> = (Y_hat - Y) ** <span class="hljs-number">2</span><br>    <span class="hljs-attribute">conv2d</span>.zero_grad()<br>    <span class="hljs-attribute">l</span>.sum().backward()<br>    <span class="hljs-attribute">conv2d</span>.weight.data[:] -= <span class="hljs-number">3</span>e-<span class="hljs-number">2</span> * conv2d.weight.grad<br>    <span class="hljs-attribute">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-attribute">print</span>(f&#x27;epoch &#123;i+<span class="hljs-number">1</span>&#125;, loss &#123;l.sum():.<span class="hljs-number">3</span>f&#125;&#x27;)<br><br></code></pre></td></tr></table></figure><h2 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h2><p>在所有侧边填充1个像素</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> torch <br><span class="hljs-attribute">from</span> torch import nn<br><br><span class="hljs-attribute">def</span> comp_conv2d(conv2d,X)；<br>    <span class="hljs-attribute">X</span>=X.reshape((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)+X.shape)#增加批量大小和通道数<br>    <span class="hljs-attribute">Y</span>=conv2d(X)<br>    <span class="hljs-attribute">return</span> Y.reshape(Y.shape[<span class="hljs-number">2</span>:])<br><br><span class="hljs-comment">#padding代表上下左右填充的像素数</span><br><span class="hljs-attribute">conv2d</span> = nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br><span class="hljs-attribute">X</span> = torch.rand(size=(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>))<br><span class="hljs-attribute">comp_conv2d</span>(conv2d,X).shape<br></code></pre></td></tr></table></figure><h2 id="多输入输出"><a href="#多输入输出" class="headerlink" title="多输入输出"></a>多输入输出</h2><p>对于彩色图像这种信息，每个像素有RGB三个通道，每个通道都是一个二维矩阵，所以输入通道数为3。<br>输出通道数可以任意设置，一般为32、64、128等。</p><h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3><p>每个通道对应一个卷积核，结果是所有通道卷积结果的和<br><img src="/img/%E5%8D%B7%E7%A7%AF%E5%B1%824.png" alt="多输入通道"></p><h3 id="多输出通道"><a href="#多输出通道" class="headerlink" title="多输出通道"></a>多输出通道</h3><p>每个输出通道对应一个卷积核，结果是每个输出通道的卷积结果的和<br><img src="/img/%E5%8D%B7%E7%A7%AF%E5%B1%825.png" alt="多输出通道"></p><h3 id="多输入多输出"><a href="#多输入多输出" class="headerlink" title="多输入多输出"></a>多输入多输出</h3><ul><li>每个通道可以识别特定模式</li><li>利用1*1卷积核,没有单通道的广度，可以多通道的融合，相当于全连接层</li></ul><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><ol><li>多输入通道互相关运算</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">corr2d_multi_in</span>(<span class="hljs-params">X,K</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(d2l.corr2d(x,k) <span class="hljs-keyword">for</span> x,k <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(X,K))<br></code></pre></td></tr></table></figure><ol start="2"><li>多输出通道互相关运算</li></ol><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">corr2d_multi_in_out</span>(X,K):<br>    return torch.<span class="hljs-built_in">stack</span>([<span class="hljs-built_in">corr2d_multi_in</span>(X,k) for k in K],<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>pytorch基础</title>
    <link href="/2025/08/17/25_08_17pytorch%E5%9F%BA%E7%A1%80/"/>
    <url>/2025/08/17/25_08_17pytorch%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="pytorch基础"><a href="#pytorch基础" class="headerlink" title="pytorch基础"></a>pytorch基础</h1><h2 id="自定义块"><a href="#自定义块" class="headerlink" title="自定义块"></a>自定义块</h2><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> torch<br><span class="hljs-title">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-title">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-class"></span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">MLP</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super().__init__()</span><br><span class="hljs-class">        self.hidden = nn.<span class="hljs-type">Linear</span>(20, 256)</span><br><span class="hljs-class">        self.out = nn.<span class="hljs-type">Linear</span>(256, 10)</span><br><span class="hljs-class"> </span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-type">X</span>):</span><br><span class="hljs-class">        return self.out(<span class="hljs-type">F</span>.<span class="hljs-title">relu</span>(<span class="hljs-title">self</span>.<span class="hljs-title">hidden</span>(<span class="hljs-type">X</span>)))</span><br><span class="hljs-class"></span><br></code></pre></td></tr></table></figure><h2 id="参数管理"><a href="#参数管理" class="headerlink" title="参数管理"></a>参数管理</h2><p>将神经网络（nn.Module）看作是一个参数容器，它包含了所有的参数（权重和偏置），以及一些方法（如前向传播）。</p><ol><li>参数访问<ul><li><code>net.state_dict()</code>：返回一个字典，包含了模型的所有参数（权重和偏置），还可以利用net[n]来提取第n层的参数</li><li><code>net[n].weight</code>：第n层的权重，<code>net[n].weight.data</code>：第n层的权重数据</li><li><code>net[n].bias</code>：第n层的偏置，<code>net[n].bias.data</code>：第n层的偏置数据</li><li><code>net.parameters()</code>：返回一个迭代器，包含了模型的所有参数</li></ul></li><li>参数初始化 <figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">init_normal</span>(m):<br>    if <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        nn.init.<span class="hljs-built_in">normal_</span>(m.weight, mean=<span class="hljs-number">0</span>, std=<span class="hljs-number">0.01</span>)<br>        nn.init.<span class="hljs-built_in">zeros_</span>(m.bias)<br><br>net.<span class="hljs-built_in">apply</span>(init_normal)#遍历所有层，对每个层应用init_normal函数<br></code></pre></td></tr></table></figure> <figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">init_constant</span>(m):<br>    if <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        nn.init.<span class="hljs-built_in">constant_</span>(m.weight, <span class="hljs-number">1</span>)<br>        nn.init.<span class="hljs-built_in">zeros_</span>(m.bias)<br><br>net.<span class="hljs-built_in">apply</span>(init_constant)<br></code></pre></td></tr></table></figure> 可以对不同层应用不同的初始化函数： <figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">init_xavier</span>(m):<br>    if <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        nn.init.<span class="hljs-built_in">xavier_uniform_</span>(m.weight)<br>        nn.init.<span class="hljs-built_in">zeros_</span>(m.bias)<br><br>def <span class="hljs-built_in">init_42</span>(m):<br>    if <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        nn.init.<span class="hljs-built_in">constant_</span>(m.weight, <span class="hljs-number">42</span>)<br>        nn.init.<span class="hljs-built_in">zeros_</span>(m.bias)<br>net[<span class="hljs-number">0</span>].<span class="hljs-built_in">apply</span>(init_xavier)<br>net[<span class="hljs-number">1</span>].<span class="hljs-built_in">apply</span>(init_42)<br></code></pre></td></tr></table></figure></li><li>参数绑定 <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">shared = nn<span class="hljs-selector-class">.Linear</span>(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>)<br>net = nn<span class="hljs-selector-class">.Sequential</span>(nn<span class="hljs-selector-class">.Linear</span>(<span class="hljs-number">4</span>, <span class="hljs-number">8</span>), nn<span class="hljs-selector-class">.ReLU</span>(),<br>                    shared, nn<span class="hljs-selector-class">.ReLU</span>(),<br>                    shared, nn<span class="hljs-selector-class">.ReLU</span>(),<br>                    nn<span class="hljs-selector-class">.Linear</span>(<span class="hljs-number">8</span>, <span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure> shared层的参数在不同层之间共享，即shared层的参数在不同层之间是指向同一个内存地址的</li></ol><h2 id="自定义层"><a href="#自定义层" class="headerlink" title="自定义层"></a>自定义层</h2><p>可以把层作为组件嵌套进更复杂的模型</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyLinear</span>(nn.<span class="hljs-title class_">Module</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):<br>        <span class="hljs-variable language_">super</span>().__init__()<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, X</span>):<br>        <span class="hljs-keyword">return</span> X - X.mean()<br></code></pre></td></tr></table></figure><p>带参数的层：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyLinear</span>(nn.<span class="hljs-title class_">Module</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, in_features, out_features</span>):<br>        <span class="hljs-variable language_">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.weight = nn.<span class="hljs-title class_">Parameter</span>(torch.randn(in_features, out_features))<br>        <span class="hljs-variable language_">self</span>.bias = nn.<span class="hljs-title class_">Parameter</span>(torch.zeros(out_features))<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, X</span>):<br>        <span class="hljs-keyword">return</span> X @ <span class="hljs-variable language_">self</span>.weight + <span class="hljs-variable language_">self</span>.bias<br></code></pre></td></tr></table></figure><p>自定义模型：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">MyMLP</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super().__init__()</span><br><span class="hljs-class">        self.linear = <span class="hljs-type">MyLinear</span>(20, 256)</span><br><span class="hljs-class">        self.out = nn.<span class="hljs-type">Linear</span>(256, 10)</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-type">X</span>):   </span><br><span class="hljs-class">        <span class="hljs-type">X</span> = self.linear(<span class="hljs-type">X</span>)  </span><br><span class="hljs-class">        return self.out(<span class="hljs-type">F</span>.<span class="hljs-title">relu</span>(<span class="hljs-type">X</span>))</span><br><span class="hljs-class">        # return self.out(<span class="hljs-type">X</span>)</span><br><span class="hljs-class">net = <span class="hljs-type">MyMLP</span>()</span><br></code></pre></td></tr></table></figure><h2 id="保存与加载"><a href="#保存与加载" class="headerlink" title="保存与加载"></a>保存与加载</h2><ol><li>加载和保存模型参数<ul><li><code>torch.save(net.state_dict(), &#39;net.params&#39;)</code>：保存模型参数</li><li><code>net.load_state_dict(torch.load(&#39;net.params&#39;))</code>：加载模型参数</li></ul></li><li>加载和保存整个模型<ul><li><code>torch.save(net, &#39;net.pt&#39;)</code>：保存整个模型</li><li><code>net = torch.load(&#39;net.pt&#39;)</code>：加载整个模型</li><li><code>net.eval()</code>：将模型设置为评估模式，即关闭dropout和batch normalization等层的训练模式</li><li><code>net.train()</code>：将模型设置为训练模式，即开启dropout和batch normalization等层的训练模式</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>LOAM代码笔记</title>
    <link href="/2025/08/16/25_08_16LOAM%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/08/16/25_08_16LOAM%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="LOAM代码笔记"><a href="#LOAM代码笔记" class="headerlink" title="LOAM代码笔记"></a>LOAM代码笔记</h1><h2 id="源码注解见本人的github"><a href="#源码注解见本人的github" class="headerlink" title="源码注解见本人的github"></a>源码注解见<a href="https://github.com/Fandy-Zhao/A-LOAM_note.git">本人的github</a></h2><h2 id="scanRegistration-cpp"><a href="#scanRegistration-cpp" class="headerlink" title="scanRegistration.cpp"></a><strong>scanRegistration.cpp</strong></h2><h3 id="目的："><a href="#目的：" class="headerlink" title="目的："></a>目的：</h3><p>实现激光点云的配准，主要包括点云的预处理、特征提取和配准过程。</p><h3 id="功能："><a href="#功能：" class="headerlink" title="功能："></a>功能：</h3><ol><li>订阅原始点云信息，对每周期点云按线序排列（原始信息是混乱的）。需要计算angle俯仰角和ori偏角，angle负责得出该点云的竖直方向的线序，ori负责的出点云的说平偏转度，两者组合成intensity。处理后的每个点云都有x,y,z,intensity四个信息。</li><li>对每周期点云进行特征提取，包括提取角点和面点。计算曲率cloudCurvature，根据曲率判断是否为角点或面点。</li><li>发布处理后的点云信息，包括角点和面点。<br><img src="/img/A_LOAM1.png" alt="流程整理"></li></ol><h2 id="laserOdometry-cpp"><a href="#laserOdometry-cpp" class="headerlink" title="laserOdometry.cpp"></a><strong>laserOdometry.cpp</strong></h2><h3 id="目的：-1"><a href="#目的：-1" class="headerlink" title="目的："></a>目的：</h3><p>通过读取scanRegistration.cpp中的信息来计算帧与帧之间的变化，最终得到里程计坐标系下的激光雷达的位姿。(前端激光里程计和位姿粗估计)</p><h3 id="功能：-1"><a href="#功能：-1" class="headerlink" title="功能："></a>功能：</h3><ol><li>订阅scanRegistration.cpp中的信息，包括角点和面点。<br>2.判断数据是否异常</li><li>将当前帧映射到上一帧，需要求解变换矩阵。<br><strong>偏原理的优化部分没太看懂，需要看一下论文再去找对应部分</strong></li><li>发布位姿（激光里程计）<br><img src="/img/A_LOAM2.png" alt="流程整理"></li></ol><h2 id="laserMapping-cpp"><a href="#laserMapping-cpp" class="headerlink" title="laserMapping.cpp"></a><strong>laserMapping.cpp</strong></h2><h3 id="目的：-2"><a href="#目的：-2" class="headerlink" title="目的："></a>目的：</h3><p>通过已经获得的激光里程计信息来消除激光里程计和地图之间的误差（也就是累计的误差），使得最终的姿态都是关于世界坐标的，并建图</p><h3 id="理解"><a href="#理解" class="headerlink" title="理解:"></a>理解:</h3><ol><li>先创建一个21<em>21</em>1的大空间，每次只关注自身周围的5<em>5</em>1的小空间。<br>2.可以通过大地图的动态调整，来使得小地图处于大地图的中心位置。</li><li>之后的数学计算不太理解，需要看一下论文再去找对应部分。</li></ol><p><img src="/img/A_LOAM3.png" alt="流程介绍"></p>]]></content>
    
    
    <categories>
      
      <category>SLAM</category>
      
      <category>LOAM</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>丢弃法  正则化</title>
    <link href="/2025/08/16/25_08_16%E4%B8%A2%E5%BC%83%E6%B3%95/"/>
    <url>/2025/08/16/25_08_16%E4%B8%A2%E5%BC%83%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a><strong>目的</strong></h2><p>在训练中加入偏差，使得训练更稳定</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><strong>训练</strong>时在隐藏层后加一层扰动，在加入噪音的基础上不改变原先的期望。在<strong>推理</strong>时不使用。</p><h2 id="内容讲解"><a href="#内容讲解" class="headerlink" title="内容讲解"></a>内容讲解</h2><p><img src="/img/dropout.png" alt="原理及方法介绍"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a><strong>代码</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">droppot_layer</span>(<span class="hljs-params">X,dropput</span>):<br>    <span class="hljs-keyword">assert</span> <span class="hljs-number">0</span> &lt;= dropput &lt;= <span class="hljs-number">1</span>   <span class="hljs-comment">#限制dropput的范围</span><br>    <span class="hljs-keyword">if</span> dropput == <span class="hljs-number">0</span>:  <span class="hljs-comment">#如果dropput为0,则</span><br>        <span class="hljs-keyword">return</span> X<br>    <span class="hljs-keyword">if</span> dropput == <span class="hljs-number">1</span>:  <span class="hljs-comment">#如果dropput为1,则</span><br>        <span class="hljs-keyword">return</span> torch.zeros_like(X)  <span class="hljs-comment">#返回与X同形状的全0张</span><br>    mask = (torch.randn(X.shape) &gt; dropput).<span class="hljs-built_in">float</span>()  <span class="hljs-comment">#生成与X同形状的mask张量（0或1 ）</span><br><br>    <span class="hljs-keyword">return</span> mask * X / (<span class="hljs-number">1.0</span> - dropput)  <span class="hljs-comment">#返回mask</span><br><br><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,num_inputs, num_outputs, num_hiddens1, num_hidden2, dropout1, dropout2</span>):<br>        <span class="hljs-built_in">super</span>(Net, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.flatten = nn.Flatten()<br>        <span class="hljs-variable language_">self</span>.linear1 = nn.Linear(num_inputs, num_hiddens1)       <span class="hljs-comment">#隐藏层1</span><br>        <span class="hljs-variable language_">self</span>.linear2 = nn.Linear(num_hiddens1, num_hidden2)      <span class="hljs-comment">#隐藏层2</span><br>        <span class="hljs-variable language_">self</span>.linear3 = nn.Linear(num_hidden2, num_outputs)       <span class="hljs-comment">#输出层</span><br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU()  <span class="hljs-comment"># 定义ReLU激活函数</span><br>        <span class="hljs-variable language_">self</span>.dropout1 = dropout1  <span class="hljs-comment">#隐藏层1的dropout率</span><br>        <span class="hljs-variable language_">self</span>.dropout2 = dropout2<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        H1 = torch.relu(<span class="hljs-variable language_">self</span>.linear1(<span class="hljs-variable language_">self</span>.flatten(X.reshape((-<span class="hljs-number">1</span>, num_inputs)))))  <span class="hljs-comment">#隐藏层1</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.training:  <span class="hljs-comment">#如果模型处于训练模式</span><br>            H1 = droppot_layer(H1, dropout1)  <span class="hljs-comment">#对隐藏层1应用dropout</span><br>        H2 = <span class="hljs-variable language_">self</span>.relu(<span class="hljs-variable language_">self</span>.linear2(H1))  <span class="hljs-comment">#隐藏层2</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.training:  <span class="hljs-comment">#如果模型处于训练模式</span><br>            H2 = droppot_layer(H2, dropout2)  <span class="hljs-comment">#对隐藏层2应用dropout</span><br>        out = <span class="hljs-variable language_">self</span>.linear3(H2)  <span class="hljs-comment">#输出层</span><br>        <span class="hljs-keyword">return</span> out<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    X = torch.arange(<span class="hljs-number">784</span>, dtype=torch.float32).reshape((<span class="hljs-number">28</span>, <span class="hljs-number">28</span>))<br>    num_inputs, num_outputs, num_hiddens1, num_hidden2 = <span class="hljs-number">784</span>,<span class="hljs-number">10</span>,<span class="hljs-number">256</span>,<span class="hljs-number">256</span><br>    dropout1, dropout2 = <span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span><br><br>    net = Net(num_inputs, num_outputs, num_hiddens1, num_hidden2, dropout1, dropout2)<br>    num_epochs, lr, batch_size = <span class="hljs-number">10</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">256</span><br>    loss = nn.CrossEntropyLoss()<br>    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)<br>    trainer = torch.optim.SGD(net.parameters(), lr=lr)<br>    d2l.train_ch3(<br>        net, train_iter, test_iter, loss, num_epochs, trainer<br>    )<br>    plt.show()  <span class="hljs-comment"># 确保脚本运行时弹出图像窗口</span><br>    <span class="hljs-built_in">print</span>(net(X))  <span class="hljs-comment"># 测试网络输出</span><br></code></pre></td></tr></table></figure><h2 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a><strong>个人理解</strong></h2><p>丢弃法是在训练时，随机舍弃几个输入值，并对其他输入做处理，即加入噪音又维持偏差为0</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>权重衰退  梯度爆炸</title>
    <link href="/2025/08/15/25_08_15%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80/"/>
    <url>/2025/08/15/25_08_15%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80/</url>
    
    <content type="html"><![CDATA[<h1 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a><strong>目的</strong></h2><p>限制权重w，防止梯度爆炸</p><h2 id="内容讲解"><a href="#内容讲解" class="headerlink" title="内容讲解"></a>内容讲解</h2><p><img src="/img/weight_decay.png" alt="原理及方法介绍"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a><strong>代码</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-comment">#生成人工数据集</span><br>n_train, n_test, num_inputs = <span class="hljs-number">20</span>, <span class="hljs-number">100</span>, <span class="hljs-number">200</span><br>batch_size = <span class="hljs-number">5</span><br>true_w, true_b = torch.ones((num_inputs, <span class="hljs-number">1</span>)) * <span class="hljs-number">0.01</span>, <span class="hljs-number">0.05</span><br>train_data = d2l.synthetic_data(true_w,true_b,n_train)<br>train_iter = d2l.load_array(train_data, batch_size)<br>test_data = d2l.synthetic_data(true_w,true_b,n_test)<br>test_iter = d2l.load_array(test_data, batch_size, is_train=<span class="hljs-literal">False</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_params</span>():<br>    w = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size=(num_inputs, <span class="hljs-number">1</span>), requires_grad=<span class="hljs-literal">True</span>)<br>    b = torch.zeros(<span class="hljs-number">1</span>, requires_grad=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> [w, b]<br><br><span class="hljs-comment"># L2范数惩罚(权重衰退的关键)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">l2_penalty</span>(<span class="hljs-params">w</span>):<br>    <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">sum</span>(w.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>)) / <span class="hljs-number">2</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">lambd</span>):<br>    w, b = init_params()<br>    net, loss = <span class="hljs-keyword">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss<br>    num_epochs, lr = <span class="hljs-number">100</span>, <span class="hljs-number">0.003</span><br><br>    <span class="hljs-comment">#训练,animator用于绘图</span><br>    animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epoch&#x27;</span>, ylabel=<span class="hljs-string">&#x27;loss&#x27;</span>, yscale=<span class="hljs-string">&#x27;log&#x27;</span>,<br>                            xlim=[<span class="hljs-number">5</span>, num_epochs], legend=[<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>])<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_iter:<br>            l = loss(net(X), y) + lambd * l2_penalty(w)<br>            l.<span class="hljs-built_in">sum</span>().backward()  <span class="hljs-comment">#计算梯度,backword()只能用于标量,所以要用sum()将l变成标量</span><br>            d2l.sgd([w, b], lr, batch_size)  <span class="hljs-comment">#使用小批量随机梯度下降迭代模型参数</span><br>        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:<br>            animator.add(epoch + <span class="hljs-number">1</span>, (d2l.evaluate_loss(net, train_iter, loss),<br>                                     d2l.evaluate_loss(net, test_iter, loss)))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;w的L2范数是：&#x27;</span>, torch.norm(w).item()) <br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment"># 训练时加权衰退</span><br>    train(lambd=<span class="hljs-number">10</span>)<br>    plt.show()  <br><br></code></pre></td></tr></table></figure><h2 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a><strong>个人理解</strong></h2><p>在训练时引用权重参数L2惩罚，可以有效限制权重w的增长，即总体损失和w的增长共同限制参数变化。因为当w变化过大时，随着深度增加，梯度呈指数级增长，会导致梯度爆炸。</p><h1 id="梯度爆炸-1"><a href="#梯度爆炸-1" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h1>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>深度学习知识树</title>
    <link href="/2025/08/14/25_08_14%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%A0%91/"/>
    <url>/2025/08/14/25_08_14%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<ol><li><p>感知机，多层感知机的实现</p></li><li><p>权重衰退  梯度爆炸</p></li><li><p>丢弃法  正则化</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>知识树</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>感知机</title>
    <link href="/2025/08/14/25_08_14%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <url>/2025/08/14/25_08_14%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<p>感知机是由输入input经过线性变化得到特定输出output(0或1)的结构<br>多层感知机引入<strong>非线性层&#x2F;隐藏层</strong>（激活函数）</p><p>下面是多层感知机手动实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><br>batch_size = <span class="hljs-number">256</span><br><span class="hljs-comment">#获取数据集</span><br>train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)<br><br>num_inputs = <span class="hljs-number">784</span>  <span class="hljs-comment">#28*28</span><br>num_outputs = <span class="hljs-number">10</span>  <span class="hljs-comment">#10个类别</span><br>num_hiddens = <span class="hljs-number">256</span>  <span class="hljs-comment">#隐藏层神经元个数(超参数)</span><br><br><span class="hljs-comment"># 训练层参数</span><br>W1 = nn.Parameter(<br>    torch.randn(num_inputs, num_hiddens, requires_grad=<span class="hljs-literal">True</span>)<span class="hljs-comment">#训练时反向传播需要梯度</span><br>)<br>b1 = nn.Parameter(<br>    torch.zeros(num_hiddens, requires_grad=<span class="hljs-literal">True</span>)<br>)<br><br>W2 = nn.Parameter(<br>    torch.randn(num_hiddens, num_outputs, requires_grad=<span class="hljs-literal">True</span>)<br>)<br>b2 = nn.Parameter(<br>    torch.zeros(num_outputs, requires_grad=<span class="hljs-literal">True</span>)<br>)<br><br>params = [W1, b1, W2, b2]<br><br><span class="hljs-comment">#实现激活函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">X</span>):<br>    a = torch.zeros_like(X)<br>    <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">max</span>(X, a)<br><br><span class="hljs-comment">#实现模型</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">net</span>(<span class="hljs-params">X</span>):<br>    X = X.reshape((-<span class="hljs-number">1</span>, num_inputs))  <span class="hljs-comment">#将输入展平</span><br>    H = relu(X @ W1 + b1)  <span class="hljs-comment">#隐藏层，@表示矩阵乘法</span><br>    <span class="hljs-keyword">return</span> H @ W2 + b2  <span class="hljs-comment">#输出层</span><br><br>num_epochs, lr = <span class="hljs-number">10</span>, <span class="hljs-number">0.1</span>  <span class="hljs-comment">#训练轮数，学习率</span><br>updater = torch.optim.SGD(params, lr=lr)  <span class="hljs-comment">#优化器</span><br>d2l.train_ch3(<br>    net, train_iter, test_iter, loss=nn.CrossEntropyLoss(),num_epochs=num_epochs,<br>    updater=updater<br>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>MIT Cheetah 3框架解读</title>
    <link href="/2025/08/14/25_08_14MIT-Cheetah-3%E6%A1%86%E6%9E%B6%E8%A7%A3%E8%AF%BB/"/>
    <url>/2025/08/14/25_08_14MIT-Cheetah-3%E6%A1%86%E6%9E%B6%E8%A7%A3%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>四足机器人</category>
      
      <category>控制框架</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>四足机器人运动学与动力学</title>
    <link href="/2025/08/12/25_08_12%E5%9B%9B%E8%B6%B3%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%BF%90%E5%8A%A8%E5%AD%A6%E4%B8%8E%E5%8A%A8%E5%8A%9B%E5%AD%A6/"/>
    <url>/2025/08/12/25_08_12%E5%9B%9B%E8%B6%B3%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%BF%90%E5%8A%A8%E5%AD%A6%E4%B8%8E%E5%8A%A8%E5%8A%9B%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="四足机器人运动学与动力学"><a href="#四足机器人运动学与动力学" class="headerlink" title="四足机器人运动学与动力学"></a>四足机器人运动学与动力学</h1><h2 id="运动学分析"><a href="#运动学分析" class="headerlink" title="运动学分析"></a>运动学分析</h2><h3 id="单腿运动学分析"><a href="#单腿运动学分析" class="headerlink" title="单腿运动学分析"></a>单腿运动学分析</h3><h4 id="正运动学分析"><a href="#正运动学分析" class="headerlink" title="正运动学分析"></a>正运动学分析</h4><p>概述：已知关节角度，求解足端位置<br>分类：三自由度串联腿，二自由度并联腿。（<strong>以三自由度串联腿为例，二自由度并联腿只需要去掉绕X轴的旋转关节,再加上一个电机角度的换算即可</strong>）<br>步骤：</p><ol><li>建立坐标系<br> 每个关节（自由度）都代表一个坐标系，以此实现腿部活动。如下图所示，可以建立三个坐标系，还有一个机体自身的坐标系<br> <img src="/img/leg_connect.png" alt="腿部连接示意图"></li><li>计算变换矩阵<br> 已知三个角度，每个关节对应一组旋转矩阵T(0,1),T(1,2),T(2,3)。<br> <strong>注意</strong>：左右腿的腿长互为相反数，便于表示</li><li>求解足端位置<br> 将旋转矩阵和基体系坐标累乘之后可以得到足端位置<br><strong>可以将三个坐标分别看作是三个角度组成的函数，引入雅各比矩阵<img src="/img/JJJ.png" alt="雅各比矩阵，对运动学正解求导"></strong></li></ol><h4 id="逆运动学"><a href="#逆运动学" class="headerlink" title="逆运动学"></a>逆运动学</h4><p>概述：已知足端位置，求解关节角度<br>步骤:<br><img src="/img/inv_single_1.png" alt="步骤一"><br><img src="/img/inv_single_2.png" alt="步骤二"><br><img src="/img/inv_single_3.png" alt="步骤三"></p><h3 id="单腿静力学分析"><a href="#单腿静力学分析" class="headerlink" title="单腿静力学分析"></a>单腿静力学分析</h3><p><strong>前提</strong>四足机器人静止，此时可以认为关节电机的功率和足端力的<strong>功率守恒</strong>（考虑做功守恒会有当前时刻做功不一样但总功一样，控制精度较差）<br><strong>公式</strong></p><ol><li>力矩乘以角加速度等于足端力乘以线速度</li><li>代入雅各比矩阵的公式化简</li><li>推出关节力矩和足端力的关系 <strong>（和运动学分析得到的雅各比矩阵有关）</strong><br><img src="/img/single_peace.png" alt="计算介绍"></li></ol><h3 id="四足运动学"><a href="#四足运动学" class="headerlink" title="四足运动学"></a>四足运动学</h3><h4 id="静止姿态的改变"><a href="#静止姿态的改变" class="headerlink" title="静止姿态的改变"></a>静止姿态的改变</h4><p><strong>目标</strong>：已知机身姿态，求关节角度<br>步骤：</p><ol><li>先假定右前足端为世界系的坐标原点，建立世界坐标系</li><li>计算四个足端相对于世界系的坐标P（s,i）&#x3D;T(s,b)P(b,i)</li><li>姿态不同，<strong>T</strong>会改变，P（b,i）已知，求解P（s,i）</li><li>由<strong>运动学逆解</strong>去推关节角度</li></ol><h4 id="足端速度"><a href="#足端速度" class="headerlink" title="足端速度"></a>足端速度</h4><p>足端速度分解：</p><ol><li>关节转动产生的速度v1(b)（用雅各比矩阵求）</li><li><strong>机身平移旋转</strong>产生的速度v2(b)（目标量）</li></ol><p>背景：机体有平移速度v0,Pi绕PO有角速度wi<br>v2(b)等于v0+wi*(p0-pi)<br>v(s)&#x3D;T(s,b)(v1(b)+v2(b))</p><h3 id="四足动力学"><a href="#四足动力学" class="headerlink" title="四足动力学"></a>四足动力学</h3>]]></content>
    
    
    <categories>
      
      <category>四足机器人</category>
      
      <category>运动学与动力学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>四足知识树整理</title>
    <link href="/2025/08/12/25_08_12%E5%9B%9B%E8%B6%B3%E7%9F%A5%E8%AF%86%E6%A0%91%E6%95%B4%E7%90%86/"/>
    <url>/2025/08/12/25_08_12%E5%9B%9B%E8%B6%B3%E7%9F%A5%E8%AF%86%E6%A0%91%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="四足知识树整理（整理中）"><a href="#四足知识树整理（整理中）" class="headerlink" title="四足知识树整理（整理中）"></a>四足知识树整理（整理中）</h1><h2 id="四足机器人大纲"><a href="#四足机器人大纲" class="headerlink" title="四足机器人大纲"></a>四足机器人大纲</h2><ol><li>控制方式：位控，<strong>力控</strong>，速控<br>作用：控制机器人静止或运动时的平衡状态</li><li>运动状态&#x2F;步态控制<br> 2.1 静态步态<br>2.1.1  <strong>trot步态(对角步态)</strong><br> 2.2 动态步态<br>2.2.1  跑步步态<br>2.2.2  飞跃步态<br> 2.3 步态规划</li><li>运动执行（和1. 控制方式相关）<br> 3.1 运动学分析<br>3.1.1  正运动学分析<br>3.1.2  逆运动学分析<br> 3.2 静力学分析<br> 3.3 动力学分析<br> 3.4 <strong>步态执行</strong></li><li>姿态估计<br> 4.1 IMU<br> 4.2卡尔曼滤波</li><li>位置获取<br> 5.1 激光雷达定位<br> 5.2 视觉定位<br> 5.3 IMU里程计定位<br> 5.4 融合定位</li><li>代码框架</li><li>路径规划<br> 6.1 比赛路径规划<br> 6.2 导航路径规划<br> 6.3 <strong>自动越野路径规划</strong></li></ol>]]></content>
    
    
    <categories>
      
      <category>四足机器人</category>
      
      <category>知识树</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/08/11/hello-world/"/>
    <url>/2025/08/11/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
  
  
  <entry>
    <title>about</title>
    <link href="/"/>
    <url>/</url>
    
    <content type="html"><![CDATA[<p>该博客用以记录自己的学习过程</p>]]></content>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/"/>
    <url>/</url>
    
    <content type="html"><![CDATA[<h1 id="Blogger"><a href="#Blogger" class="headerlink" title="Blogger"></a>Blogger</h1><p>博客备份</p>]]></content>
    
  </entry>
  
  
  
  <entry>
    <title>about</title>
    <link href="/"/>
    <url>/</url>
    
    <content type="html"><![CDATA[<p>该博客用以记录自己的学习过程</p>]]></content>
    
  </entry>
  
  
  
</search>
