<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>强化学习</title>
    <link href="/2025/09/08/25_09_08%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <url>/2025/09/08/25_09_08%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><ul><li><p><strong>value-based</strong> and <strong>policy-based</strong>:<br>  <strong>value-based</strong>:基于状态价值和动作价值来更新策略(不是直接学习策略)，原理类似于先求出每个状态-动作对的价值,据此来选择最佳策略，有点类似动态规划（DP）。又分为value iterationn和policy iteration。</p><p>  对于<strong>value iteration</strong>,先进行<strong>policy update</strong>,将状态价值迭代至收敛即最优并更新策略，再进行<strong>value update</strong>，根据更新后的策略来更新状态价值。</p><p>  对于<strong>policy iteration</strong>,先进行<strong>policy evalution</strong>，一般利用<strong>蒙特卡罗方法MC</strong>和<strong>时序差分方法TD（SARSA,Q-Learning）<strong>来求出策略的状态价值，再进行</strong>policy improvement</strong>对当前策略进行更新，一般都会使用<strong>ε-贪婪探索</strong></p><p>  <strong>policy-based</strong>：直接基于策略更新，将策略看成函数，最优策略就是一个使用使得损失函数最小的参数的函数，一般使用策略梯度更新。常见算法:REINFORCE,Actor-Critic,TRPO,PPO,TD3等</p></li><li><p><strong>off-online</strong> and <strong>on-line</strong>:<br>  <strong>off-online</strong>:学习的策略和采样的行为策略不一样，高效（经验池回放，重复性采样）但分布不一致（需要重要性采样），有Q-Learning,DQN<br>  <strong>on-online</strong>:学习的策略和采样的行为策略一样，稳定但信息利用率低，有SARSA，REINFORCE，PPO</p></li></ul><h1 id="针对四足的强化学习项目"><a href="#针对四足的强化学习项目" class="headerlink" title="针对四足的强化学习项目"></a>针对四足的强化学习项目</h1><h2 id="项目框架"><a href="#项目框架" class="headerlink" title="项目框架"></a>项目框架</h2><p>基于现成框架，可以更好进行二次开发</p><ul><li><a href="https://github.com/leggedrobotics/legged_gym">legged_gym</a>基于Isaac_Gym，提供四足机器人的环境，包括模拟和真实环境</li><li><a href="https://github.com/leggedrobotics/rsl_rl">rsl_rl</a>基于legged_gym，提供强化学习算法的实现，包括PPO，TRPO，TD3等</li></ul><h2 id="强化学习算法"><a href="#强化学习算法" class="headerlink" title="强化学习算法"></a>强化学习算法</h2><p>目前大都基于PPO，TRPO的Actor-Critic框架</p><h2 id="学习感受"><a href="#学习感受" class="headerlink" title="学习感受"></a>学习感受</h2><ul><li>对于强化学习的理论知识要求不高，只需要知道大体的算法原理及其基本特点，尤其是PPO之类的AC算法</li><li>需要学习对应的强化学习框架，目前比较流行的框架是legged_gym和rsl_rl（基于Isaac_gym），可以去尝试跑通开源项目，理清框架，方便进行二次开发</li></ul>]]></content>
    
    
    <categories>
      
      <category>四足机器人</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>MPC学习</title>
    <link href="/2025/09/08/25_09_08MPC%E5%AD%A6%E4%B9%A0/"/>
    <url>/2025/09/08/25_09_08MPC%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="学习记录"><a href="#学习记录" class="headerlink" title="学习记录"></a>学习记录</h1><ol><li>学习MPC的基本原理，以及求解方法–<strong>二次规划</strong></li><li>发现和四足机器人的联系不大，还是不知道四足如何用MPC来控制</li><li>阅读足式机器人MPC综述<a href="https://doi.org/10.1080/01691864.2023.2168134">Model predictive control of legged and humanoid robots: models and algorithms</a>，了解到不同的动力学建模方式，不同的求解方式</li></ol><h1 id="笔记整理"><a href="#笔记整理" class="headerlink" title="笔记整理"></a>笔记整理</h1>]]></content>
    
    
    <categories>
      
      <category>四足机器人</category>
      
      <category>MPC</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>SVD分解</title>
    <link href="/2025/08/29/25_08_29SVD%E5%88%86%E8%A7%A3/"/>
    <url>/2025/08/29/25_08_29SVD%E5%88%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="奇异值分解SVD"><a href="#奇异值分解SVD" class="headerlink" title="奇异值分解SVD"></a>奇异值分解SVD</h1><h2 id="定义与性质"><a href="#定义与性质" class="headerlink" title="定义与性质"></a>定义与性质</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>奇异值分解将任何给定矩阵分解为三个矩阵的乘积：一个正交的左奇异向量矩阵、一个对角的奇异值矩阵和一个正交的右奇异向量矩阵。将数据集的奇异值表征按重要性排列，舍弃不重要的特征向量，达到降维的目的，从而找出数据中的主成分。矩阵的奇异值分解可以看做是矩阵数据压缩的一种方法，即用因子分解的方式近似地表示原始矩阵，这种近似是在平方损失意义下的最优近似。它在应用如数据降维、信息检索、信号处理和图像压缩等领域具有重要作用。</p><h3 id="定义定理"><a href="#定义定理" class="headerlink" title="定义定理"></a>定义定理</h3><p><img src="/img/SVD1.png"></p><h2 id="紧奇异值分解和截断奇异值分解"><a href="#紧奇异值分解和截断奇异值分解" class="headerlink" title="紧奇异值分解和截断奇异值分解"></a>紧奇异值分解和截断奇异值分解</h2><h3 id="紧奇异值分解"><a href="#紧奇异值分解" class="headerlink" title="紧奇异值分解"></a>紧奇异值分解</h3><p><img src="/img/SVD2.png"></p><h3 id="截断奇异值分解"><a href="#截断奇异值分解" class="headerlink" title="截断奇异值分解"></a>截断奇异值分解</h3><p><img src="/img/SVD3.png"></p><h2 id="几何含义"><a href="#几何含义" class="headerlink" title="几何含义"></a>几何含义</h2><p><img src="/img/SVD4.png"><br><img src="/img/SVD5.png"></p>]]></content>
    
    
    <categories>
      
      <category>SLAM</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>转置卷积</title>
    <link href="/2025/08/22/25_08_22_%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF/"/>
    <url>/2025/08/22/25_08_22_%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF/</url>
    
    <content type="html"><![CDATA[<h1 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h1><p>使用神经网络的过程中,我们经常需要上采样(up-sampling)来提高低分辨率图片的分辨率.</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><ul><li>插值<ul><li>最近邻插值</li><li>双线性插值</li><li>双立方插值</li></ul></li><li>转置卷积</li></ul><h1 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h1><p>转置卷积是上采样的一种方法,它的主要思想是将卷积操作的过程反过来进行,从而实现上采样的目的。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>FCN网络</title>
    <link href="/2025/08/22/25_08_22FCN%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/08/22/25_08_22FCN%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="全连接神经网络FCN"><a href="#全连接神经网络FCN" class="headerlink" title="全连接神经网络FCN"></a>全连接神经网络FCN</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li>深度神经网络做语义分割的基础</li><li>用转置卷积层替换池化层和全连接层，可以实现每个像素的预测</li></ul><h2 id="转置卷积Transpose-Convolution"><a href="#转置卷积Transpose-Convolution" class="headerlink" title="转置卷积Transpose Convolution"></a>转置卷积Transpose Convolution</h2><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="/img/FCN%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png" alt="FCN网络结构"></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>语义分割</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ArkTS基础项目</title>
    <link href="/2025/08/22/25_08_22ArkTS%E5%9F%BA%E7%A1%80%E9%A1%B9%E7%9B%AE/"/>
    <url>/2025/08/22/25_08_22ArkTS%E5%9F%BA%E7%A1%80%E9%A1%B9%E7%9B%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="创建项目，软件图标"><a href="#创建项目，软件图标" class="headerlink" title="创建项目，软件图标"></a>创建项目，软件图标</h1><p>修改配置文件module.json5<br><img src="/img/%E4%BF%AE%E6%94%B9%E8%AF%B4%E6%98%8E.png" alt="展示"></p><h1 id="页面跳转–Navigation组件"><a href="#页面跳转–Navigation组件" class="headerlink" title="页面跳转–Navigation组件"></a>页面跳转–Navigation组件</h1><p><img src="/img/%E5%AF%BC%E8%88%AA%E7%BB%84%E4%BB%B6.png" alt="展示"><br>详情可以查看官方文档 </p>]]></content>
    
    
    <categories>
      
      <category>鸿蒙</category>
      
      <category>ArkTS</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ArkTS基础语法1</title>
    <link href="/2025/08/21/25_08_21ArkTS%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/"/>
    <url>/2025/08/21/25_08_21ArkTS%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><ul><li>字符串（string）</li><li>数字（number）</li><li>状态（boolean）</li></ul><h2 id="数据声明"><a href="#数据声明" class="headerlink" title="数据声明"></a>数据声明</h2><p><code>let 变量名: 数据类型 = 值;</code></p><h2 id="数组声明"><a href="#数组声明" class="headerlink" title="数组声明"></a>数组声明</h2><p><code>let 数组名: 数据类型[] = [值1, 值2, 值3];</code><br><code>let titles: string[] = [&#39;标题1&#39;, &#39;标题2&#39;, &#39;标题3&#39;];</code></p><h2 id="对象声明"><a href="#对象声明" class="headerlink" title="对象声明"></a>对象声明</h2><p>对象可以包含多个属性，每个属性由键值对组成，键是属性名，值是属性值。<br><code>let 对象名: &#123; 属性名: 数据类型, 属性名: 数据类型 &#125; = &#123; 属性名: 值, 属性名: 值 &#125;;</code><br><code>let person: &#123; name: string, age: number &#125; = &#123; name: &#39;张三&#39;, age: 18 &#125;;</code></p><h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><h2 id="函数声明"><a href="#函数声明" class="headerlink" title="函数声明"></a>函数声明</h2><p>函数声明使用<code>function</code>关键字，后面跟函数名、参数列表和函数</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs arcade"><span class="hljs-keyword">function</span> 函数名(<span class="hljs-params">参数<span class="hljs-number">1</span>: 数据类型, 参数<span class="hljs-number">2</span>: 数据类型</span>)&#123;<br>  <span class="hljs-comment">// 函数体</span><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="箭头函数"><a href="#箭头函数" class="headerlink" title="箭头函数"></a>箭头函数</h2><p>箭头函数是函数的一种简写形式，使用<code>=&gt;</code>符号来定义。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-keyword">let</span> 函数名 = <span class="hljs-function">(<span class="hljs-params">参数<span class="hljs-number">1</span>: 数据类型, 参数<span class="hljs-number">2</span>: 数据类型</span>) =&gt;</span> &#123;<br>  <span class="hljs-comment">// 函数体</span><br>&#125;<br></code></pre></td></tr></table></figure><h1 id="ArkUI组件"><a href="#ArkUI组件" class="headerlink" title="ArkUI组件"></a>ArkUI组件</h1><p>利用组件来开发UI界面</p><h2 id="组件分类"><a href="#组件分类" class="headerlink" title="组件分类"></a>组件分类</h2><ul><li>容器组件：布局<br><code>组件名（）&#123;&#125;</code></li><li>内容组件：内容<br><code>组件名（）</code></li></ul><h2 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h2><p>属性是组件的特征，用于描述组件的状态或行为。<br>属性可以在组件声明时设置，也可以在组件使用时动态设置。<br>属性的设置方式是在组件名后面使用<code>()</code>来设置，属性名和属性值之间使用<code>:</code>来分隔。</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs 1c">组件名（属性名<span class="hljs-punctuation">:</span> 属性值）&#123;<br>  <span class="hljs-comment">// 组件内容</span><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="通用属性"><a href="#通用属性" class="headerlink" title="通用属性"></a>通用属性</h3><ul><li>width：组件的宽度</li><li>height：组件的高度</li><li>backgroundColor：组件的背景颜色</li><li>fontColor：组件的字体颜色</li><li>fontSize：组件的字体大小</li><li>fontWeight：组件的字体粗细</li></ul><h3 id="特殊属性"><a href="#特殊属性" class="headerlink" title="特殊属性"></a>特殊属性</h3><h4 id="文本属性"><a href="#文本属性" class="headerlink" title="文本属性"></a>文本属性</h4><ul><li>fontSize (字体大小)      数值（单位fp）</li><li>fontColor (字体颜色)     色值</li><li>fontWeight (字体粗细)    100~900</li><li>textAlign (文本对齐方式) TextAlign.left | TextAlign.center | TextAlign.right</li></ul><h4 id="内外边距属性"><a href="#内外边距属性" class="headerlink" title="内外边距属性"></a>内外边距属性</h4><p><img src="/img/%E5%86%85%E5%A4%96%E8%BE%B9%E8%B7%9D.png" alt="介绍"></p><h4 id="边框属性"><a href="#边框属性" class="headerlink" title="边框属性"></a>边框属性</h4><p>为组件添加边框效果</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs maxima">.<span class="hljs-built_in">border</span>(&#123;<br>    <span class="hljs-built_in">width</span>:粗细,<br>    <span class="hljs-built_in">color</span>:颜色<br>    <span class="hljs-built_in">radius</span>:圆角<br>    <span class="hljs-built_in">style</span>:边框线条样式<br>&#125;)<br></code></pre></td></tr></table></figure><h4 id="安全区属性"><a href="#安全区属性" class="headerlink" title="安全区属性"></a>安全区属性</h4><p>扩充组件安全区，防止内容被系统栏遮挡。</p><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nsis">.expandSafeArea([SafeAreaType.<span class="hljs-params">SYSTEM</span>],[SafeAreaType.<span class="hljs-literal">TOP</span>,SafeAreaType.<span class="hljs-literal">BOTTOM</span>])<br></code></pre></td></tr></table></figure><h3 id="属性调用"><a href="#属性调用" class="headerlink" title="属性调用"></a>属性调用</h3><p><code>.属性名(属性值)</code></p><h2 id="图像组件"><a href="#图像组件" class="headerlink" title="图像组件"></a>图像组件</h2><p><code>Image(图像资源路径)</code></p><h3 id="路径选择"><a href="#路径选择" class="headerlink" title="路径选择"></a>路径选择</h3><ul><li>本地路径，图片存放在\src\main\ets\resources\base\media目录下,调用时<code>$r(app.media.图片名) </code></li><li>网络路径，图片存放在网络上的地址，调用时<code>(&#39;网址名&#39;)</code></li></ul><h1 id="语句"><a href="#语句" class="headerlink" title="语句"></a>语句</h1><h2 id="if分支语句"><a href="#if分支语句" class="headerlink" title="if分支语句"></a>if分支语句</h2><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs aspectj"><span class="hljs-keyword">if</span> (条件) &#123;<br>  <span class="hljs-comment">// 代码块</span><br>&#125; <span class="hljs-function"><span class="hljs-keyword">else</span> <span class="hljs-title">if</span> <span class="hljs-params">(条件)</span> </span>&#123;<br>  <span class="hljs-comment">// 代码块</span><br>&#125; <span class="hljs-keyword">else</span> &#123;<br>  <span class="hljs-comment">// 代码块</span><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="条件表达式"><a href="#条件表达式" class="headerlink" title="条件表达式"></a>条件表达式</h2><p>条件表达式是根据条件的真假来决定执行哪个表达式。<br><code>条件？条件成立的表达式：条件不成立的表达式</code></p><h2 id="条件渲染"><a href="#条件渲染" class="headerlink" title="条件渲染"></a>条件渲染</h2><p>根据逻辑条件结果，渲染不同的组件。</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs aspectj"><span class="hljs-keyword">if</span> (条件) &#123;<br>  <span class="hljs-comment">// 组件1</span><br>&#125; <span class="hljs-function"><span class="hljs-keyword">else</span> <span class="hljs-title">if</span> <span class="hljs-params">(条件)</span> </span>&#123;<br>  <span class="hljs-comment">// 组件2</span><br>&#125; <span class="hljs-keyword">else</span> &#123;<br>  <span class="hljs-comment">// 组件3</span><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="循环渲染"><a href="#循环渲染" class="headerlink" title="循环渲染"></a>循环渲染</h2><p>根据循环条件，重复执行相同的代码块。</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-selector-tag">ForEach</span>(数组, (<span class="hljs-attribute">item</span>: 数据类型, <span class="hljs-attribute">index</span>: number) =&gt; &#123;<br>  <span class="hljs-comment">// 代码块,item是数组中的每一个元素,index是数组中的每一个元素的索引</span><br>&#125;)<br></code></pre></td></tr></table></figure><h2 id="状态管理"><a href="#状态管理" class="headerlink" title="状态管理"></a>状态管理</h2><p>状态变量：使用装饰器装饰，状态变量数据的变化(<strong>事件</strong>触发 )，需要UI界面实时渲染<br>初始化：<code>@Local 状态变量名: 数据类型 = 值;</code>需要声明<code>@ComponentV2</code></p><h2 id="自定义封装函数"><a href="#自定义封装函数" class="headerlink" title="自定义封装函数"></a>自定义封装函数</h2><p>使用@Builder装饰器，将函数封装为组件，实现组件的复用。</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-variable">@Builder</span><br>函数名(参数列表) &#123;<br>  <span class="hljs-comment">// 组件内容</span><br>&#125;<br></code></pre></td></tr></table></figure><p>调用：<code>this.函数名(参数列表)</code>(写在组件里面)</p>]]></content>
    
    
    <categories>
      
      <category>鸿蒙</category>
      
      <category>ArkTS</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>鸿蒙工程讲解</title>
    <link href="/2025/08/21/25_08_21%E9%B8%BF%E8%92%99%E5%B7%A5%E7%A8%8B%E8%AE%B2%E8%A7%A3/"/>
    <url>/2025/08/21/25_08_21%E9%B8%BF%E8%92%99%E5%B7%A5%E7%A8%8B%E8%AE%B2%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="工程目录结构（Stage模型）"><a href="#工程目录结构（Stage模型）" class="headerlink" title="工程目录结构（Stage模型）"></a>工程目录结构（Stage模型）</h1><h2 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs css">├── AppScope &gt; app<span class="hljs-selector-class">.json5</span>：应用的全局配置信息，详见app<span class="hljs-selector-class">.json5</span>配置文件。<br>├── entry：HarmonyOS工程模块，编译构建生成一个HAP包。（默认项目入口模块）<br>    ├── <span class="hljs-attribute">src</span> &gt; <span class="hljs-selector-tag">main</span> &gt; ets：用于存放ArkTS源码。<br>    ├── <span class="hljs-attribute">src</span> &gt; <span class="hljs-selector-tag">main</span> &gt; ets &gt; entryability：应用/服务的入口。<br>    ├── <span class="hljs-attribute">src</span> &gt; <span class="hljs-selector-tag">main</span> &gt; ets &gt; entrybackupability：应用提供扩展的备份恢复能力。<br>    ├── <span class="hljs-attribute">src</span> &gt; <span class="hljs-selector-tag">main</span> &gt; ets &gt; pages：应用/服务包含的页面。<br>    <br>    ├── <span class="hljs-attribute">src</span> &gt; <span class="hljs-selector-tag">main</span> &gt; resources：用于存放应用/服务所用到的资源文件，如图形、多媒体、字符串、布局文件等。关于资源文件，详见资源分类与访问。<br>    ├── <span class="hljs-attribute">src</span> &gt; <span class="hljs-selector-tag">main</span> &gt; module<span class="hljs-selector-class">.json5</span>：模块配置文件。主要包含HAP包的配置信息、应用/服务在具体设备上的配置信息以及应用/服务的全局配置信息。具体的配置文件说明，详见module<span class="hljs-selector-class">.json5</span>配置文件。<br>    ├── build-profile<span class="hljs-selector-class">.json5</span>：当前的模块信息 、编译信息配置项，包括buildOption、targets配置等。<br>    ├── hvigorfile<span class="hljs-selector-class">.ts</span>：模块级编译构建任务脚本。<br>    ├── obfuscation-rules<span class="hljs-selector-class">.txt</span>：混淆规则文件。混淆开启后，在使用Release模式进行编译时，会对代码进行编译、混淆及压缩处理，保护代码资产。详见开启代码混淆。<br>    ├── oh-package<span class="hljs-selector-class">.json5</span>：用来描述包名、版本、入口文件（类型声明文件）和依赖项等信息。<br>├── oh_modules：用于存放三方库依赖信息。<br>├── build-profile<span class="hljs-selector-class">.json5</span>：工程级配置信息，包括签名signingConfigs、产品配置products等。其中products中可配置当前运行环境，默认为HarmonyOS。<br>├── hvigorfile<span class="hljs-selector-class">.ts</span>：工程级编译构建任务脚本。<br>├── oh-package<span class="hljs-selector-class">.json5</span>：主要用来描述全局配置，如：依赖覆盖（overrides）、依赖关系重写（overrideDependencyMap）和参数化配置（parameterFile）等<br></code></pre></td></tr></table></figure><h1 id="应用程序包"><a href="#应用程序包" class="headerlink" title="应用程序包"></a>应用程序包</h1><p>用户应用程序是指运行在设备操作系统之上，为用户提供特定服务的程序，简称“应用”。一个应用所对应的软件包文件，称为“应用程序包”。</p><h2 id="多Module设计机制"><a href="#多Module设计机制" class="headerlink" title="多Module设计机制"></a>多Module设计机制</h2><ul><li><strong>支持模块化开发</strong>： 一个应用通常包含多种功能，将不同的功能特性按模块来划分和管理是一种良好的设计方式。在开发过程中，开发者可以将每个功能模块作为一个独立的Module进行开发，Module中可以包含源代码、资源文件、第三方库、配置文件等，每一个Module可以独立编译，实现特定的功能。这种模块化、松耦合的应用管理方式有利于应用的开发、维护与扩展。</li><li><strong>支持多设备适配</strong>： 一个应用往往需要适配多种设备类型，在采用多Module设计的应用中，每个Module都会标注所支持的设备类型。Module支持的设备类型不同，有的支持全部类型，有的仅支持特定类型（例如平板）。在应用市场分发应用包时，可以根据设备类型进行精准筛选和匹配，从而合理组合和部署不同的包到对应的设备上。</li></ul><h2 id="Module类型"><a href="#Module类型" class="headerlink" title="Module类型"></a>Module类型</h2><p>Module按照使用场景可以分为两种类型：</p><ul><li><p>Ability类型的Module： 用于实现应用的功能和特性。每一个Ability类型的Module编译后，会生成一个以.hap为后缀的文件，称为HAP（Harmony Ability Package）包。HAP包可以独立安装和运行，是应用安装的基本单位，一个应用可以包含一个或多个HAP包，包含的HAP包分为以下两种类型。</p><ul><li>entry类型的Module：应用的主模块，包含应用的入口界面、入口图标和主功能特性，编译后生成entry类型的HAP。每一个应用分发到同一类型的设备上的应用程序包，只能包含唯一一个entry类型的HAP，也可以不包含。</li><li>feature类型的Module：应用的动态特性模块，编译后生成feature类型的HAP。一个应用中可以包含一个或多个feature类型的HAP，也可以不包含。</li></ul></li><li><p>Library类型的Module： 用于实现代码和资源的共享。同一个Library类型的Module可以被其他的Module多次引用，合理地使用该类型的Module，能够降低开发和维护成本。Library类型的Module分为Static和Shared两种类型，编译后生成共享包。</p></li><li><p>Library类型的Module</p></li><li><p>Static Library：静态共享库。编译后生成一个以.har为后缀的文件，即静态共享包HAR（Harmony Archive）。 <strong>注意</strong>：编译HAR时，建议开启混淆能力，保护代码资产。</p></li><li><p>Shared Library：动态共享库。编译后生成一个以.hsp为后缀的文件，即动态共享包HSP（Harmony Shared Package）。</p></li></ul><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>实际上，Shared Library编译后除了会生成一个.hsp文件，还会生成一个.har文件。这个.har文件中包含了HSP对外导出的接口，应用中的其他模块需要通过.har文件来引用HSP的功能。为了表述方便，通常认为编译Shared Library后会生成HSP。</p><h1 id="Stage模型应用程序包结构"><a href="#Stage模型应用程序包结构" class="headerlink" title="Stage模型应用程序包结构"></a>Stage模型应用程序包结构</h1><h2 id="开发态包结构"><a href="#开发态包结构" class="headerlink" title="开发态包结构"></a>开发态包结构</h2><p>工程结构主要包含的文件类型及用途如下(ModuleName指的是entry这类文件夹)：</p><ul><li>配置文件：包括应用级配置信息、以及Module级配置信息<ul><li>AppScope &gt; app.json5：app.json5配置文件，用于声明应用的全局配置信息，比如应用Bundle名称、应用名称、应用图标、应用版本号等。</li><li>ModuleName &gt; src &gt; main &gt; module.json5：module.json5配置文件，用于声明Module基本信息、支持的设备类型、所含的组件信息、运行所需申请的权限等。</li></ul></li><li>ArkTS源码文件<ul><li>ModuleName &gt; src &gt; main &gt; ets：用于存放Module的ArkTS源码文件（.ets文件）。</li></ul></li><li>资源文件：包括应用级资源文件、以及Module级资源文件，支持图形、多媒体、字符串、布局文件等，详见资源分类与访问<ul><li>AppScope &gt; resources ：用于存放应用需要用到的资源文件。</li><li>ModuleName &gt; src &gt; main &gt; resources ：用于存放该Module需要用到的资源文件。</li></ul></li><li>其他配置文件：用于编译构建，包括构建配置文件、编译构建任务脚本、混淆规则文件、依赖的共享包信息等<ul><li>build-profile.json5：工程级或Module级的构建配置文件，包括应用签名、产品配置等。</li><li>hvigorfile.ts：工程级或Module级的编译构建任务脚本，开发者可以自定义编译构建工具版本、控制构建行为的配置参数。</li><li>obfuscation-rules.txt：混淆规则文件。混淆开启后，在使用Release模式进行编译时，会对代码进行编译、混淆及压缩处理，保护代码资产。</li><li>oh-package.json5：用于存放依赖库的信息，包括所依赖的三方库和共享包。</li></ul></li></ul><h2 id="编译态包结构"><a href="#编译态包结构" class="headerlink" title="编译态包结构"></a>编译态包结构</h2><p>不同类型的Module编译后会生成对应的HAP、HAR、HSP等文件，开发态视图与编译态视图的对照关系如下<br><img src="/img/%E5%BC%80%E5%8F%91to%E7%BC%96%E8%AF%91.png" alt="开发态与编译态的工程结构视图"></p><h1 id="HAP开发"><a href="#HAP开发" class="headerlink" title="HAP开发"></a>HAP开发</h1><p>HAP（Harmony Ability Package）是应用安装和运行的基本单元。HAP包是由代码、资源、第三方库、配置文件等打包生成的模块包</p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><ul><li>entry类型的HAP：应用的主模块，包含应用的入口界面、入口图标和主功能特性，编译后生成entry类型的HAP。每一个应用分发到同一类型的设备上的应用程序包，只能包含唯一一个entry类型的HAP，也可以不包含。</li><li>feature类型的HAP：应用的动态特性模块，编译后生成feature类型的HAP。一个应用中可以包含一个或多个feature类型的HAP，也可以不包含。</li></ul><h2 id="约束限制"><a href="#约束限制" class="headerlink" title="约束限制"></a>约束限制</h2><ul><li>不支持导出接口和ArkUI组件，给其他模块使用。</li><li>多HAP场景下，App Pack包中同一设备类型的所有HAP中最多只能包含一个Entry类型的HAP，也可以不包含；Feature类型的HAP可以包含一个或者多个，也可以不包含。</li><li>多HAP场景下，同一应用中的所有HAP的配置文件中的bundleName、versionCode、versionName、minCompatibleVersionCode、debug、minAPIVersion、targetAPIVersion、apiReleaseType相同，同一设备类型的所有HAP对应的moduleName标签必须唯一。HAP打包生成App Pack包时，会对上述参数配置进行校验。</li><li>多HAP场景下，同一应用的所有HAP、HSP的签名证书要保持一致。上架应用市场是以App Pack形式上架，应用市场分发时会将所有HAP从App Pack中拆分出来，同时对所有HAP进行重签名，以保证签名证书的一致性。在调试阶段，开发者通过命令行或DevEco Studio将HAP安装到设备上时，要保证所有HAP签名证书一致，否则会出现安装失败的问题，签名操作请参考应用&#x2F;元服务签名。</li></ul><h1 id="HAR开发"><a href="#HAR开发" class="headerlink" title="HAR开发"></a>HAR开发</h1><p>HAR（Harmony Archive）是静态共享包，可以包含代码、C++库、资源和配置文件。通过HAR可以实现多个模块或多个工程共享ArkUI组件、资源等相关代码。</p><h2 id="约束限制-1"><a href="#约束限制-1" class="headerlink" title="约束限制"></a>约束限制</h2><ul><li>HAR不支持在设备上单独安装或运行，只能作为应用模块的依赖项被引用。</li><li>HAR不支持在配置文件中声明ExtensionAbility组件。从API version 14开始，支持在配置文件中声明UIAbility组件，配置UIAbility的方法参考在模块中添加Ability，拉起HAR中UIAbility的方式与启动应用内的UIAbility方法相同。</li></ul><figure class="highlight objectivec"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs objectivec">说明<br>如果使用startAbility接口拉起HAR中的<span class="hljs-built_in">UIAbility</span>，接口参数中的moduleName取值需要为依赖该HAR的HAP/HSP的moduleName。<br></code></pre></td></tr></table></figure><ul><li>HAR不支持在配置文件中声明pages页面，但是可以包含pages页面，并通过Navigation跳转的方式进行跳转。</li><li>HAR不支持引用AppScope目录中的资源。在编译构建时，AppScope中的内容不会打包到HAR中，因此会导致HAR资源引用失败。</li><li>由于HSP仅支持应用内共享，如果HAR依赖了HSP，则该HAR文件仅支持应用内共享，不支持发布到二方仓或三方仓供其他应用使用，否则会导致编译失败。</li><li>多包（HAP&#x2F;HSP）引用相同的HAR时，会造成多包间代码和资源的重复拷贝，从而导致应用包变大。</li><li>HAR可以依赖其他HAR或者HSP，但不支持循环依赖，也不支持依赖传递。</li><li>HAP引用HAR时，在编译构建过程中系统会自动合并两者的权限配置。因此开发者无需在HAP和HAR中重复申请相同权限。</li></ul><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs css">说明<br>循环依赖：例如有三个HAR，HAR-<span class="hljs-selector-tag">A</span>、HAR-<span class="hljs-selector-tag">B</span>和HAR-C，循环依赖指HAR-<span class="hljs-selector-tag">A</span>依赖HAR-<span class="hljs-selector-tag">B</span>，HAR-<span class="hljs-selector-tag">B</span>依赖HAR-C，HAR-C又依赖HAR-<span class="hljs-selector-tag">A</span>。<br>依赖传递：例如有三个HAR，HAR-<span class="hljs-selector-tag">A</span>、HAR-<span class="hljs-selector-tag">B</span>和HAR-C，依赖关系是HAR-<span class="hljs-selector-tag">A</span>依赖HAR-<span class="hljs-selector-tag">B</span>，HAR-<span class="hljs-selector-tag">B</span>依赖HAR-C。不支持传递依赖指HAR-<span class="hljs-selector-tag">A</span>可以使用HAR-<span class="hljs-selector-tag">B</span>的方法和组件，但是HAR-<span class="hljs-selector-tag">A</span>不能直接使用HAR-C的方法和组件。<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>鸿蒙</category>
      
      <category>工程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>LOAM_Livox论文笔记</title>
    <link href="/2025/08/20/25_08_20LOAM-Livox%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/08/20/25_08_20LOAM-Livox%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/LOAM_Livox1.png" alt="LOAM_Livox"></p><h1 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h1><ul><li>在非常有限的FoV中进行特征提取和选择</li><li>鲁棒的离群点剔除：激光雷达的扫描线束和扫描轨迹分布不均匀</li><li>移动对象过滤</li><li>运动失真补偿</li></ul><h1 id="POINTS-SELECTION-AND-FEATURE-EXTRACTION"><a href="#POINTS-SELECTION-AND-FEATURE-EXTRACTION" class="headerlink" title="POINTS SELECTION AND FEATURE EXTRACTION"></a>POINTS SELECTION AND FEATURE EXTRACTION</h1><p>受激光雷达在真实物理世界的限制，需要在原始点云信息中提取出好的特征点</p><ol><li>分类<br> 按照特定指标选择点<ol><li>对于每个点计算下列指标</li></ol><ul><li>Depth D：点到雷达的距离</li><li>laser deflection angle φ ：X轴与激光线夹角</li><li>intensity I ：反射强度，R是物体反射率，由雷达测量</li><li>incident angle θ ：入射角，激光层与测量点周围的局部平面之间的角度<img src="/img/LOAM_Livox4.png" alt="入射角示意图"></li></ul><ol start="2"><li>计算公式<br> <img src="/img/LOAM_Livox2.png" alt="1"><br> <img src="/img/LOAM_Livox3.png" alt="2"></li><li>舍弃原则</li></ol><ul><li>靠近雷达视野边缘。此时扫描轨迹不可靠</li><li>intensity太大或太小。都会降低测量的准确率</li><li>入射角接近π或0的点。会产生光斑导致测量的是一片范围的均值而不是某一特定点</li><li>被障碍物遮挡的点。会被误判为边缘点</li></ul></li><li>特征提取<ul><li>计算局部平滑度（应该是曲率）来提取边缘点和平面点</li><li>引入反射率作为判断依据，如果由一点和周围点的反射率区别过大，被视为（材料）边缘点</li></ul></li></ol><h1 id="ITERATIVE-POSE-OPTIMIZATION-迭代姿态优化"><a href="#ITERATIVE-POSE-OPTIMIZATION-迭代姿态优化" class="headerlink" title="ITERATIVE POSE OPTIMIZATION  迭代姿态优化"></a>ITERATIVE POSE OPTIMIZATION  迭代姿态优化</h1><p>受雷达非重复扫描的影响，不能利用重复点的匹配来获得位姿</p><ol><li>边对边的残差计算<ol><li>在当前帧中选取一点Pl，再从全局地图中选取距离Pl最近的5个点：先将Pl映射到地图系，在从地图系的KDtree里面选取5个点</li><li>以当前帧的最后一点Pw的雷达姿态为准，计算当前帧到地图的转换矩阵来实现投影</li><li>确保选取的5个Pi和点Pw在同一条直线上    </li><li>残差计算<br> <img src="/img/LOAM_Livox5.png" alt="3和4计算思路"></li></ol></li><li>面对面的残差计算<br> 和边对边的残差计算思路相同，就是计算是否是同一平面上由不同<br> <img src="/img/LOAM_Livox6.png" alt="计算思路"></li><li>帧内运动补偿<ol><li>分段处理：同一帧分成几部分，独立进行处理。相当于缩短运动时间，减轻运动带来的影响，分开处理还可以并行计算，缩短计算时间</li><li>线性插值：假设在一帧中运动是均匀的，可以利用最后一个点的姿态变化去估计之前点的转换矩阵<img src="/img/LOAM_Livox7.png" alt="线性插值计算细节"></li></ol></li><li>离群点剔除和移动对象过滤<br> 避免移动物体对扫描精度产生影响<br> 思路：在每次迭代重新计算所有特征点对应的残差，先做迭代优化残差，最后在优化后结果中去除前20%最大的点（边缘点和平面点分开进行处理）</li></ol><h1 id="个人评价"><a href="#个人评价" class="headerlink" title="个人评价"></a>个人评价</h1><p>亮点：</p><ul><li>特征提取时引入反射强度作为评价依据</li><li>迭代姿态优化可以解决雷达扫描不重复的问题</li><li>将优化后残差较大的点作为运动点</li></ul>]]></content>
    
    
    <categories>
      
      <category>SLAM</category>
      
      <category>LOAM</category>
      
      <category>论文笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>SSD,YOLO物体检测算法</title>
    <link href="/2025/08/19/25_08_19SSD%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/"/>
    <url>/2025/08/19/25_08_19SSD%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h1><h2 id="锚框选择"><a href="#锚框选择" class="headerlink" title="锚框选择"></a>锚框选择</h2><p>以每个像素为中心，生成多个锚框</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>多次卷积得到不同大小的图（关注范围不同），每次都有一遍锚框选择<br><img src="/img/SSD.png" alt="SSD模型结构"></p><h1 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h1><h2 id="锚框选择-1"><a href="#锚框选择-1" class="headerlink" title="锚框选择"></a>锚框选择</h2><p>将图片均匀分为S*S个锚框，每个锚框去预测多个种类边缘框（避免单个锚框框住多个种类目标）</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>物体检测</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>RCNN物体检测算法</title>
    <link href="/2025/08/19/25_08_19RCNN%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/"/>
    <url>/2025/08/19/25_08_19RCNN%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h1><p><img src="/img/RCNN1.png" alt="示意图"></p><ul><li>启发式搜索来选择锚框</li><li>通过CNN提取特征</li><li>基于锚框的分类和回归<ul><li>分类: SVM</li><li>回归: 线性回归,来获得锚框和目标框的偏移量</li></ul></li></ul><h2 id="兴趣区域（ROI）池化层"><a href="#兴趣区域（ROI）池化层" class="headerlink" title="兴趣区域（ROI）池化层"></a>兴趣区域（ROI）池化层</h2><p>将不同大小的特征图转换为固定大小的特征图</p><ul><li>输入: 特征图, 锚框</li><li>输出: 固定大小的特征图</li><li>过程:<ul><li>对每个锚框, 均匀分割为固定数量的区域</li><li>对每个区域, 取像素最大值</li><li>输出固定大小的特征图<br><img src="/img/RCNN2.png" alt="ROI池化"></li></ul></li></ul><h1 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h1><h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><ul><li>改变了启发式搜索的位置</li><li>引入了CNN的特征提取层，在ROI池化层之前。在原图像中启发式搜索获取锚框，映射到CNN的输出上，再交给CNN。可以先抽取全局的特征再进行ROI池化，而不是先ROI池化再过CNN.</li></ul><h1 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h1><h2 id="改进-相对于-Fast-R-CNN"><a href="#改进-相对于-Fast-R-CNN" class="headerlink" title="改进(相对于 Fast R-CNN)"></a>改进(相对于 Fast R-CNN)</h2><ul><li>将启发式搜索替换为区域建议网络RPN（Region Proposal Network）<ul><li>输入: 图像</li><li>输出: 锚框</li><li>过程:<ul><li>使用CNN提取特征图</li><li>生成锚框</li><li>预测每个锚框的分类和回归偏移量</li><li>非极大值抑制(NMS)<ul><li>输入: 锚框, 分类分数</li><li>输出: 保留的锚框</li><li>过程:<ul><li>对每个类别, 按分类分数排序</li><li>从高到低遍历, 保留当前锚框, 并移除与当前锚框重叠度高的锚框</li></ul></li></ul></li></ul></li></ul></li></ul><p><img src="/img/RPN.png" alt="RPN"></p><h1 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h1><h2 id="改进-相对于-Faster-R-CNN"><a href="#改进-相对于-Faster-R-CNN" class="headerlink" title="改进(相对于 Faster R-CNN)"></a>改进(相对于 Faster R-CNN)</h2><ul><li>引入了掩码预测层FCN( Fully Convolutional Network)<ul><li>作用：利用像素级的标号</li><li>输入: 锚框</li><li>输出: 掩码</li><li>过程:<ul><li>对每个锚框, 预测一个二进制掩码</li><li>对每个像素, 预测它是否属于物体</li></ul></li></ul></li><li>ROI池化层被替换成ROI align层<ul><li>作用：更准确的提取特征</li><li>输入: 特征图, 锚框</li><li>输出: 固定大小的特征图</li><li>过程:<ul><li>对每个锚框, 均匀分割为固定数量的区域（每个区域的大小相同，交叉的像素点会被按权重拆开）</li><li>对每个区域, 取像素最大值</li><li>输出固定大小的特征图<br><img src="/img/MaskRCNN.png" alt="Mask R-CNN"></li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>物体检测</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ResNet网络</title>
    <link href="/2025/08/18/25_08_18ResNet%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/08/18/25_08_18ResNet%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>问题：随着网络层数的增加，模型的性能会出现饱和甚至下降,增加更多的层反而不能增加精度<br>想法：<br>使得每一层都包含之前层的信息，从而避免信息丢失，即使深度加深，可以保证精度不会下降太多</p><h2 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h2><p>将本次输入加到本次输出中<br><img src="/img/%E6%AE%8B%E5%B7%AE%E5%9D%97.png" alt="残差块"></p><h2 id="ResNet块"><a href="#ResNet块" class="headerlink" title="ResNet块"></a>ResNet块</h2><p><img src="/img/ResNet%E5%9D%97.png" alt="ResNet块"><br>残差块可以进行替换组合</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>批量归一化</title>
    <link href="/2025/08/18/25_08_18%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <url>/2025/08/18/25_08_18%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>底层参数的变化会导致顶层的较大变化，导致训练效果不好</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>批量归一化（Batch Normalization），对每一层的输入进行归一<br>化处理，使得每一层的输入分布保持稳定。</p><ul><li>固定每个小批量的均值和方差</li><li>对每个小批量的输入进行归一化处理</li><li>引入可学习的缩放参数和偏移参数，用于调整归一化后的输出</li></ul><h2 id="批量归一化层"><a href="#批量归一化层" class="headerlink" title="批量归一化层"></a>批量归一化层</h2><ul><li>可学习的缩放参数和偏移参数</li><li>作用位置：全连接层和卷积层的输出，激活函数之前</li><li>作用：归一化输入，加速训练，正则化模型</li></ul><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs css">import torch <br><span class="hljs-selector-tag">from</span> torch import nn<br><span class="hljs-selector-tag">from</span> d2l import torch as d2l<br><br>def batch_norm(<span class="hljs-attribute">X</span>,gamma,beta,moving_mean,moving_var,eps,momentum):<br>    if not torch.<span class="hljs-built_in">is_grad_enabled</span>():<br>        X_hat = (X - moving_mean) / torch.<span class="hljs-built_in">sqrt</span>(moving_var + eps)<br>    else:<br>        assert <span class="hljs-built_in">len</span>(X.shape) in (<span class="hljs-number">2</span>, <span class="hljs-number">4</span>), <span class="hljs-string">&quot;Batch normalization only supports 2D or 4D inputs&quot;</span><br>        if <span class="hljs-built_in">len</span>(X.shape) == <span class="hljs-number">2</span>:<br>            mean = X.<span class="hljs-built_in">mean</span>(dim=<span class="hljs-number">0</span>)<br>            var = ((X - mean) ** <span class="hljs-number">2</span>).<span class="hljs-built_in">mean</span>(dim=<span class="hljs-number">0</span>)<br>        else:<br>            mean = X.<span class="hljs-built_in">mean</span>(dim=(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), keepdim=True)<br>            var = ((X - mean) ** <span class="hljs-number">2</span>).<span class="hljs-built_in">mean</span>(dim=(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), keepdim=True)<br><br>        X_hat = (X - mean) / torch.<span class="hljs-built_in">sqrt</span>(var + eps)<br>        moving_mean = momentum * moving_mean + (<span class="hljs-number">1.0</span> - momentum) * mean# 移动平均<br><br>        moving_var = momentum * moving_var + (<span class="hljs-number">1.0</span> - momentum) * var  # 移动平均<br><br>    Y = gamma * X_hat + beta<br>    return Y, moving_mean, moving_var<br><br>class <span class="hljs-built_in">BatchNorm</span>(nn.Module):<br>    def <span class="hljs-built_in">__init__</span>(self, num_features, num_dims):<br>        <span class="hljs-built_in">super</span>().<span class="hljs-built_in">__init__</span>()<br>        if num_dims == <span class="hljs-number">2</span>:<br>            shape = (<span class="hljs-number">1</span>, num_features)<br>        else:<br>            shape = (<span class="hljs-number">1</span>, num_features, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        self.gamma = nn.<span class="hljs-built_in">Parameter</span>(torch.<span class="hljs-built_in">ones</span>(shape))<br>        self.beta = nn.<span class="hljs-built_in">Parameter</span>(torch.<span class="hljs-built_in">zeros</span>(shape))<br>        self.moving_mean = torch.<span class="hljs-built_in">zeros</span>(shape)<br>        self.moving_var = torch.<span class="hljs-built_in">ones</span>(shape)<br><br>    def <span class="hljs-built_in">forward</span>(self, X):<br>        if self.moving_mean.device != X.device:<br>            self.moving_mean = self.moving_mean.<span class="hljs-built_in">to</span>(X.device)<br>            self.moving_var = self.moving_var.<span class="hljs-built_in">to</span>(X.device)<br>        Y, self.moving_mean, self.moving_var = <span class="hljs-built_in">batch_norm</span>(<br>            X, self.gamma, self.beta, self.moving_mean, self.moving_var,<br>            eps=<span class="hljs-number">1</span>e-<span class="hljs-number">5</span>, momentum=<span class="hljs-number">0.9</span>)<br>        return Y<br><br>if __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    # 测试BatchNorm<br>    # X = torch.<span class="hljs-built_in">randn</span>(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)  # <span class="hljs-number">4</span>个样本，<span class="hljs-number">3</span>个通道，<span class="hljs-number">2</span>x2的特征图<br>    # batch_norm_layer = <span class="hljs-built_in">BatchNorm</span>(num_features=<span class="hljs-number">3</span>, num_dims=<span class="hljs-number">4</span>)<br>    # Y = <span class="hljs-built_in">batch_norm_layer</span>(X)<br>    # <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, Y.shape)  # 应该是 (<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><br><br>    #应用于LeNet<br>    net = nn.<span class="hljs-built_in">Sequential</span>(<br>        nn.<span class="hljs-built_in">Conv2d</span>(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>),<br>        <span class="hljs-built_in">BatchNorm</span>(<span class="hljs-number">6</span>, num_dims=<span class="hljs-number">4</span>),<br>        nn.<span class="hljs-built_in">Sigmoid</span>(),<br>        nn.<span class="hljs-built_in">MaxPool2d</span>(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>        nn.<span class="hljs-built_in">Conv2d</span>(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>),<br>        <span class="hljs-built_in">BatchNorm</span>(<span class="hljs-number">16</span>, num_dims=<span class="hljs-number">4</span>),<br>        nn.<span class="hljs-built_in">Sigmoid</span>(),<br>        nn.<span class="hljs-built_in">MaxPool2d</span>(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>        nn.<span class="hljs-built_in">Flatten</span>(),<br>        nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">16</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">120</span>),<br>        <span class="hljs-built_in">BatchNorm</span>(<span class="hljs-number">120</span>, num_dims=<span class="hljs-number">2</span>),<br>        nn.<span class="hljs-built_in">Sigmoid</span>(),<br>        nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>),<br>        <span class="hljs-built_in">BatchNorm</span>(<span class="hljs-number">84</span>, num_dims=<span class="hljs-number">2</span>),<br>        nn.<span class="hljs-built_in">Sigmoid</span>(),<br>        nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br>    )<br><br>    lr, num_epochs = <span class="hljs-number">1.0</span>, <span class="hljs-number">10</span><br>    batch_size = <span class="hljs-number">256</span><br>    train_iter, test_iter = d2l.<span class="hljs-built_in">load_data_fashion_mnist</span>(batch_size)<br>    d2l.<span class="hljs-built_in">train_ch6</span>(net, train_iter, test_iter, num_epochs, lr, d2l.<span class="hljs-built_in">try_gpu</span>())<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>GoogleNet网络</title>
    <link href="/2025/08/18/25_08_18GoogleNet%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/08/18/25_08_18GoogleNet%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h1><h2 id="Inception模块"><a href="#Inception模块" class="headerlink" title="Inception模块"></a>Inception模块</h2><p>多个路径从不同·层面抽取信息，然后在输出通道合并，即使不同路径的输出通道数不同，也可以通过1*1卷积层来进行通道数的匹配。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="/img/GoogleNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png" alt="GoogleNet网络结构"></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>NiN网络</title>
    <link href="/2025/08/18/25_08_18NiN%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/08/18/25_08_18NiN%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>问题：对于全连接层，在卷积层之后的第一个全连接层的输入通道数是非常大的，这个时候如果使用全连接层，会导致参数数量非常多，从而导致模型的过拟合。</p><h2 id="NiN网络"><a href="#NiN网络" class="headerlink" title="NiN网络"></a>NiN网络</h2><h3 id="NiN块"><a href="#NiN块" class="headerlink" title="NiN块"></a>NiN块</h3><ul><li>一个卷积层后跟两个全连接层</li><li>全连接层：1*1卷积层</li></ul><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><ol><li>无全连接层</li><li>交替使用NiN块和步幅为2的最大池化层（逐步减小高宽和增大通道数）</li><li>最后使用全局平均池化层的到输出（输入通道是类别数）<br><img src="/img/NiN%E6%9E%B6%E6%9E%84.png" alt="NiN架构"></li></ol><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h1 id="定义NiN块"><a href="#定义NiN块" class="headerlink" title="定义NiN块"></a>定义NiN块</h1><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch<br><span class="hljs-keyword">from</span> torch import nn<br><span class="hljs-keyword">from</span> d2l import torch as d2l<br><br>def nin_block(in_channels, out_channels, kernel_size, <span class="hljs-attribute">stride</span>=1, <span class="hljs-attribute">padding</span>=0):<br>    return nn.Sequential(<br>        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),<br>        nn.ReLU(),<br>        nn.Conv2d(out_channels, out_channels, <span class="hljs-attribute">kernel_size</span>=1),  # 1x1卷积<br>        nn.ReLU(),<br>        nn.Conv2d(out_channels, out_channels, <span class="hljs-attribute">kernel_size</span>=1),  # 1x1卷积<br>        nn.ReLU()<br>    )<br><br><br>def nin(<span class="hljs-attribute">num_classes</span>=10):<br>    return nn.Sequential(<br>        nin_block(1, 96, <span class="hljs-attribute">kernel_size</span>=11, <span class="hljs-attribute">stride</span>=4, <span class="hljs-attribute">padding</span>=0),  # 卷积层1<br>        nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),  # 池化层1<br>        nin_block(96, 256, <span class="hljs-attribute">kernel_size</span>=5, <span class="hljs-attribute">stride</span>=1, <span class="hljs-attribute">padding</span>=2),  # 卷积层2<br>        nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),  # 池化层2<br>        nin_block(256, 384, <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=1, <span class="hljs-attribute">padding</span>=1),  # 卷积层3<br>        nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),  # 池化层3<br>        nn.Dropout(0.5),  # 丢弃层<br>        nin_block(384, 10, <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=1, <span class="hljs-attribute">padding</span>=1),  # 卷积层4, 输出层<br>        nn.AdaptiveAvgPool2d((1, 1)),  # 自适应平均池化<br>        nn.Flatten()  # 展平层<br>    )<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    net = nin()<br>    <br>    # 测试网络结构<br>    # X = torch.randn(1, 1, 224, 224)  # 输入图像的形状<br>    # <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    #     X = layer(X)  # 前向传播<br>    #     <span class="hljs-built_in">print</span>(layer.__class__.__name__, <span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br><br>    lr, num_epochs = 0.01, 10<br>    batch_size = 128<br>    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, <span class="hljs-attribute">resize</span>=224)<br>    d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>VGG网络</title>
    <link href="/2025/08/18/25_08_18VGG%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/08/18/25_08_18VGG%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>AlexNet对于LeNet的提升得益于网络的更深更大，网络更深更大的方法：</p><ul><li>更多的全连接层</li><li>更多的卷积层</li><li>将卷积层分成块（VGG）</li></ul><p>和AlexNet的区别：<br>使用尺寸更小的3×3卷积核串联来替代大卷积核11×11,7×7这样的大尺寸卷积核，引入块设计思想，在相同的感受野的情况下，多个串联非线性能力更强，描述能力更强</p><h2 id="VGG思路"><a href="#VGG思路" class="headerlink" title="VGG思路"></a>VGG思路</h2><p>VGG网络的思路是将卷积层分成多个块，每个块包含多个卷积层和一个池化层，最后串联接多个全连接层。<br><img src="/img/VGG%E6%9E%B6%E6%9E%84.png" alt="VGG网络结构"></p><ul><li>VGG-16 13个卷积层与3个全连接层</li><li>VGG-19 16个卷积层与3个全连接层</li></ul><h2 id="VGG实现"><a href="#VGG实现" class="headerlink" title="VGG实现"></a>VGG实现</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs css">import torch<br><span class="hljs-selector-tag">from</span> torch import nn<br><span class="hljs-selector-tag">from</span> d2l import torch as d2l<br><br># 定义VGG块<br>def vgg_block(num_convs, in_channels, out_channels):<br>    layers = []<br>    for _ in <span class="hljs-built_in">range</span>(num_convs):<br>        layers.<span class="hljs-built_in">append</span>(nn.<span class="hljs-built_in">Conv2d</span>(in_channels, out_channels, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>))<br>        layers.<span class="hljs-built_in">append</span>(nn.<span class="hljs-built_in">ReLU</span>())<br>        in_channels = out_channels#更新输入通道数<br>    layers.<span class="hljs-built_in">append</span>(nn.<span class="hljs-built_in">MaxPool2d</span>(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>    return nn.<span class="hljs-built_in">Sequential</span>(*layers)<br><br>def <span class="hljs-built_in">vgg</span>(conv_arch, num_classes=<span class="hljs-number">10</span>):<br>    conv_blocks = []<br>    in_channels = <span class="hljs-number">1</span>  # 输入通道数<br>    for (num_convs, out_channels) in conv_arch:<br>        conv_blocks.<span class="hljs-built_in">append</span>(<span class="hljs-built_in">vgg_block</span>(num_convs, in_channels, out_channels))<br>        in_channels = out_channels<br>    return nn.<span class="hljs-built_in">Sequential</span>(<br>        #卷积单元<br>        *conv_blocks,<br>        #全连接单元<br>        nn.<span class="hljs-built_in">Flatten</span>(),<br>        nn.<span class="hljs-built_in">Linear</span>(in_channels * <span class="hljs-number">7</span> * <span class="hljs-number">7</span>, <span class="hljs-number">4096</span>),  # 假设输入图像大小为<span class="hljs-number">224</span>x224<br>        nn.<span class="hljs-built_in">ReLU</span>(),<br>        nn.<span class="hljs-built_in">Dropout</span>(<span class="hljs-number">0.5</span>),<br>        nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>),<br>        nn.<span class="hljs-built_in">ReLU</span>(),<br>        nn.<span class="hljs-built_in">Dropout</span>(<span class="hljs-number">0.5</span>),<br>        nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">4096</span>, num_classes)<br>    )<br><br>if __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    conv_arch = [(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">512</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">512</span>)]  # VGG架构<br>    net = <span class="hljs-built_in">vgg</span>(conv_arch)<br>    <br>    # 测试网络结构<br>    # X = torch.<span class="hljs-built_in">randn</span>(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)  # 输入图像的形状<br>    # for layer in net:<br>    #     X = <span class="hljs-built_in">layer</span>(X)  # 前向传播<br>    #     <span class="hljs-built_in">print</span>(layer.__class__.__name__, <span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br><br>    batch_size = <span class="hljs-number">128</span><br>    train_iter, test_iter = d2l.<span class="hljs-built_in">load_data_fashion_mnist</span>(batch_size, resize=<span class="hljs-number">224</span>)<br>    lr, num_epochs = <span class="hljs-number">0.01</span>, <span class="hljs-number">10</span><br>    d2l.<span class="hljs-built_in">train_ch6</span>(net, train_iter, test_iter, num_epochs, lr, d2l.<span class="hljs-built_in">try_gpu</span>())<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>AlexNet网络</title>
    <link href="/2025/08/18/25_08_18AlexNet%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/08/18/25_08_18AlexNet%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><h2 id="改进（基于LeNet）"><a href="#改进（基于LeNet）" class="headerlink" title="改进（基于LeNet）"></a>改进（基于LeNet）</h2><ol><li>采用ReLU激活函数（解决了梯度消失问题）</li><li>丢弃法</li><li>MaxPooling</li><li>数据增强</li></ol><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>和LeNet相比，网络结构更复杂，层和核更大<br><img src="/img/Alexnet1.png" alt="网络结构"><br><img src="/img/Alexnet2.png" alt="网络结构-多次卷积"><br><img src="/img/Alexnet3.png" alt="网络结构-全连接层"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch<br><span class="hljs-keyword">from</span> torch import nn<br><span class="hljs-keyword">from</span> d2l import torch as d2l<br><br>net = nn.Sequential(<br>    #卷积单元<br>    nn.Conv2d(1,96,<span class="hljs-attribute">kernel_size</span>=11,stride=4, <span class="hljs-attribute">padding</span>=1), nn.ReLU(),<br>    nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),<br>    nn.Conv2d(96, 256, <span class="hljs-attribute">kernel_size</span>=5, <span class="hljs-attribute">padding</span>=2), nn.ReLU(),<br>    nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),<br>    nn.Conv2d(256, 384, <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">padding</span>=1), nn.ReLU(),<br>    nn.Conv2d(384, 384, <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">padding</span>=1), nn.ReLU(),<br>    nn.Conv2d(384, 256, <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">padding</span>=1), nn.ReLU(),<br>    nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),<br>    #全连接单元<br>    nn.Flatten(),<br>    nn.Linear(6400, 4096), nn.ReLU(),<br>    nn.Dropout(0.5),     #丢弃层<br>    nn.Linear(4096, 4096), nn.ReLU(),   <br>    nn.Dropout(0.5),<br>    nn.Linear(4096, 10)<br>)<br><br><span class="hljs-comment"># X = torch.randn(1, 1, 224, 224)  # 输入图像的形状</span><br><span class="hljs-comment"># for layer in net:</span><br><span class="hljs-comment">#     X = layer(X)  # 前向传播</span><br><span class="hljs-comment">#     print(layer.__class__.__name__, &#x27;output shape:\t&#x27;, X.shape)</span><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    batch_size = 128<br>    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, <span class="hljs-attribute">resize</span>=224)<br>    lr, num_epochs = 0.01, 10<br>    d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>池化层</title>
    <link href="/2025/08/18/25_08_18%E6%B1%A0%E5%8C%96%E5%B1%82/"/>
    <url>/2025/08/18/25_08_18%E6%B1%A0%E5%8C%96%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>卷积对位置信息的敏感度较高，而池化层对位置信息的敏感度较低。<br>池化层的作用是降低卷积层的敏感度，减少计算量，防止过拟合。</p><h2 id="池化方式"><a href="#池化方式" class="headerlink" title="池化方式"></a>池化方式</h2><ol><li><strong>最大池化</strong>: 取池化窗口内的最大值作为输出。</li><li><strong>平均池化</strong>: 取池化窗口内的平均值作为输出。</li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pool2d</span>(<span class="hljs-params">X, pool_size, mode = <span class="hljs-string">&#x27;max&#x27;</span></span>):<br>    p_h, p_w = pool_size<br>    Y = torch.zeros((X.shape[<span class="hljs-number">0</span>] - p_h + <span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>] - p_w + <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">0</span>]):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">1</span>]):<br>            <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;max&#x27;</span>:<br>                Y[i,j] = X[i:i+p_h,j:j+p_w].<span class="hljs-built_in">max</span>()<br>            <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;avg&#x27;</span>:<br>                Y[i,j] = X[i:i+p_h,j:j+p_w].mean()<br>    <span class="hljs-keyword">return</span> Y<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>LOAM论文笔记</title>
    <link href="/2025/08/18/25_08_18LOAM%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/08/18/25_08_18LOAM%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="LOAM-Lidar-Odometry-and-Mapping-in-Real-time-笔记"><a href="#LOAM-Lidar-Odometry-and-Mapping-in-Real-time-笔记" class="headerlink" title="LOAM: Lidar Odometry and Mapping in Real-time 笔记"></a>LOAM: Lidar Odometry and Mapping in Real-time 笔记</h1><h2 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h2><p>实时性的位姿估计定位和建图算法，既有高频率低精度里程计进行运动估计，又有低频率高精度点云匹配来地图建图。</p><h2 id="算法一-LIDAR-ODOMETRY"><a href="#算法一-LIDAR-ODOMETRY" class="headerlink" title="算法一  LIDAR ODOMETRY"></a>算法一  LIDAR ODOMETRY</h2><ol><li><strong>特征点匹配</strong>：通过计算曲率c来判断点是否为角点或面点。在c的计算中引入S来消除距离影响，将每一次扫描分成多个相同的子区域，保证特征点的均匀分布。还要注意排除几种特殊情况。</li><li><strong>寻找对应特征点集合</strong>：<ul><li>先将前一时刻的点云投影到当前时刻。</li><li>利用两个时刻的点云信息来进行角点和面点的匹配，使用最近邻搜索相邻特征点，角点进行点到直线的距离计算，面点进行点到平面的距离计算。</li></ul></li><li><strong>运动动作估计</strong>：<ul><li>通过角点和面点的匹配来计算变换矩阵T。</li><li>知道在一段时间内的变化矩阵T,假设运动是均匀的，可以利用时间进行线性插值，得到当前时刻的变换矩阵T。</li><li>利用变换矩阵T来根据投影量更新当前时刻的点云坐标。</li><li>进而将该特征点的距离公式作为损失函数，利用优化算法来求解变换矩阵T，使得距离趋近于0.</li></ul></li><li><strong>伪代码</strong><br><img src="/img/LOAM%E4%BC%AA%E4%BB%A3%E7%A0%811.png" alt="伪代码"></li></ol><h2 id="算法二-LIDAR-MAPPING"><a href="#算法二-LIDAR-MAPPING" class="headerlink" title="算法二  LIDAR MAPPING"></a>算法二  LIDAR MAPPING</h2>]]></content>
    
    
    <categories>
      
      <category>SLAM</category>
      
      <category>LOAM</category>
      
      <category>论文笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>拉普拉斯变换，带通滤波器变换手稿整理</title>
    <link href="/2025/08/17/25_08_17%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A2%EF%BC%8C%E5%B8%A6%E9%80%9A%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%8F%98%E6%8D%A2%E6%89%8B%E7%A8%BF%E6%95%B4%E7%90%86/"/>
    <url>/2025/08/17/25_08_17%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A2%EF%BC%8C%E5%B8%A6%E9%80%9A%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%8F%98%E6%8D%A2%E6%89%8B%E7%A8%BF%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="拉普拉斯变换，带通滤波器变换手稿整理"><a href="#拉普拉斯变换，带通滤波器变换手稿整理" class="headerlink" title="拉普拉斯变换，带通滤波器变换手稿整理"></a>拉普拉斯变换，带通滤波器变换手稿整理</h1><p>拉普拉斯变换，滤波器<br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A21.jpg" alt="1"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A22.jpg" alt="2"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A23.jpg" alt="3"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A24.jpg" alt="4"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A25.jpg" alt="5"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A26.jpg" alt="6"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A27.jpg" alt="7"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A28.jpg" alt="8"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A29.jpg" alt="9"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A210.jpg" alt="10"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A211.jpg" alt="11"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A212.jpg" alt="12"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A213.jpg" alt="13"><br><img src="/img/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A214.jpg" alt="14"></p>]]></content>
    
    
    <categories>
      
      <category>无人机</category>
      
      <category>姿态解算</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>傅里叶变换手稿整理</title>
    <link href="/2025/08/17/25_08_17%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E6%89%8B%E7%A8%BF%E6%95%B4%E7%90%86/"/>
    <url>/2025/08/17/25_08_17%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E6%89%8B%E7%A8%BF%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="傅里叶变换手稿整理"><a href="#傅里叶变换手稿整理" class="headerlink" title="傅里叶变换手稿整理"></a>傅里叶变换手稿整理</h1><p>傅里叶变换详解<br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(1).jpg" alt="1"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(2).jpg" alt="2"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(3).jpg" alt="3"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(4).jpg" alt="4"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(5).jpg" alt="5"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(6).jpg" alt="6"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(7).jpg" alt="7"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(8).jpg" alt="8"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(9).jpg" alt="9"><br><img src="/img/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2(10).jpg" alt="10"></p>]]></content>
    
    
    <categories>
      
      <category>无人机</category>
      
      <category>姿态解算</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>姿态解算手稿整理</title>
    <link href="/2025/08/17/25_08_17%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97%E6%89%8B%E7%A8%BF%E6%95%B4%E7%90%86/"/>
    <url>/2025/08/17/25_08_17%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97%E6%89%8B%E7%A8%BF%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="手稿整理"><a href="#手稿整理" class="headerlink" title="手稿整理"></a>手稿整理</h1><p>从一开始的姿态表示（欧拉角、四元数、旋转矩阵），到滤波用的拉普拉斯变换，再到控制PID。<br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF1.jpg" alt="1"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF2.jpg" alt="2"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF3.jpg" alt="3"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF4.jpg" alt="4"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF5.jpg" alt="5"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF6.jpg" alt="6"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF7.jpg" alt="7"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF8.jpg" alt="8"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF9.jpg" alt="9"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF10.jpg" alt="10"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF11.jpg" alt="11"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF12.jpg" alt="12"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF13.jpg" alt="13"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF14.jpg" alt="14"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF15.jpg" alt="15"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF16.jpg" alt="16"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E6%89%8B%E7%A8%BF17.jpg" alt="17"></p>]]></content>
    
    
    <categories>
      
      <category>无人机</category>
      
      <category>姿态解算</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>姿态解算资料整理</title>
    <link href="/2025/08/17/25_08_17%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/"/>
    <url>/2025/08/17/25_08_17%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="资料整理"><a href="#资料整理" class="headerlink" title="资料整理"></a>资料整理</h1><p><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_01.png" alt="1"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_02.png" alt="2"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_03.png" alt="3"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_04.png" alt="4"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_05.png" alt="5"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_06.png" alt="6"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_07.png" alt="7"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_08.png" alt="8"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_09.png" alt="9"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_10.png" alt="10"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_11.png" alt="11"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_12.png" alt="12"><br><img src="/img/%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A7%BF%E6%80%81%E8%A7%A3%E7%AE%97_13.png" alt="13"></p>]]></content>
    
    
    <categories>
      
      <category>无人机</category>
      
      <category>姿态解算</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>卷积层</title>
    <link href="/2025/08/17/25_08_17%E5%8D%B7%E7%A7%AF%E5%B1%82/"/>
    <url>/2025/08/17/25_08_17%E5%8D%B7%E7%A7%AF%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h1><h2 id="分类原则"><a href="#分类原则" class="headerlink" title="分类原则"></a>分类原则</h2><ul><li>平移不变性：分类器对输入的平移不变性，即输入的图像平移后，分类器的输出不变。</li><li>局部性：分类器关注输入的局部性</li></ul><h2 id="卷积层是一个特殊的全连接层"><a href="#卷积层是一个特殊的全连接层" class="headerlink" title="卷积层是一个特殊的全连接层"></a>卷积层是一个特殊的全连接层</h2><ol><li>设计原理：<br><img src="/img/%E5%8D%B7%E7%A7%AF%E5%B1%821.png" alt="平移不变性"><br><img src="/img/%E5%8D%B7%E7%A7%AF%E5%B1%822.png" alt="局部性"></li><li>对全连接层进行变形：<br><img src="/img/%E5%8D%B7%E7%A7%AF%E5%B1%823.png" alt="变形"></li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p><strong>二维互相关和二维卷积的实现:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">corr2d</span>(<span class="hljs-params">X, K</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span><br>    h, w = K.shape<br>    Y = torch.zeros((X.shape[<span class="hljs-number">0</span>] - h + <span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>] - w + <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">0</span>]):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">1</span>]):<br>            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="hljs-built_in">sum</span>()<br>    <span class="hljs-keyword">return</span> Y<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Conv2D</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;计算二维卷积层&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, kernel_size</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.weight = nn.Parameter(torch.rand(kernel_size))<br>        <span class="hljs-variable language_">self</span>.bias = nn.Parameter(torch.zeros(<span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> corr2d(X, <span class="hljs-variable language_">self</span>.weight) + <span class="hljs-variable language_">self</span>.bias<br><br></code></pre></td></tr></table></figure><p><strong>学习由X生成Y的卷积核:</strong></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">conv2d</span> = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, kernel_size=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), bias=False)<br><br><span class="hljs-attribute">X</span> = X.reshape((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>))<br><span class="hljs-attribute">Y</span> = Y.reshape((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>))<br><br><span class="hljs-attribute">for</span> i in range(<span class="hljs-number">10</span>):<br>    <span class="hljs-attribute">Y_hat</span> = conv2d(X)<br>    <span class="hljs-attribute">l</span> = (Y_hat - Y) ** <span class="hljs-number">2</span><br>    <span class="hljs-attribute">conv2d</span>.zero_grad()<br>    <span class="hljs-attribute">l</span>.sum().backward()<br>    <span class="hljs-attribute">conv2d</span>.weight.data[:] -= <span class="hljs-number">3</span>e-<span class="hljs-number">2</span> * conv2d.weight.grad<br>    <span class="hljs-attribute">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-attribute">print</span>(f&#x27;epoch &#123;i+<span class="hljs-number">1</span>&#125;, loss &#123;l.sum():.<span class="hljs-number">3</span>f&#125;&#x27;)<br><br></code></pre></td></tr></table></figure><h2 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h2><p>在所有侧边填充1个像素</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> torch <br><span class="hljs-attribute">from</span> torch import nn<br><br><span class="hljs-attribute">def</span> comp_conv2d(conv2d,X)；<br>    <span class="hljs-attribute">X</span>=X.reshape((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)+X.shape)#增加批量大小和通道数<br>    <span class="hljs-attribute">Y</span>=conv2d(X)<br>    <span class="hljs-attribute">return</span> Y.reshape(Y.shape[<span class="hljs-number">2</span>:])<br><br><span class="hljs-comment">#padding代表上下左右填充的像素数</span><br><span class="hljs-attribute">conv2d</span> = nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br><span class="hljs-attribute">X</span> = torch.rand(size=(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>))<br><span class="hljs-attribute">comp_conv2d</span>(conv2d,X).shape<br></code></pre></td></tr></table></figure><h2 id="多输入输出"><a href="#多输入输出" class="headerlink" title="多输入输出"></a>多输入输出</h2><p>对于彩色图像这种信息，每个像素有RGB三个通道，每个通道都是一个二维矩阵，所以输入通道数为3。<br>输出通道数可以任意设置，一般为32、64、128等。</p><h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3><p>每个通道对应一个卷积核，结果是所有通道卷积结果的和<br><img src="/img/%E5%8D%B7%E7%A7%AF%E5%B1%824.png" alt="多输入通道"></p><h3 id="多输出通道"><a href="#多输出通道" class="headerlink" title="多输出通道"></a>多输出通道</h3><p>每个输出通道对应一个卷积核，结果是每个输出通道的卷积结果的和<br><img src="/img/%E5%8D%B7%E7%A7%AF%E5%B1%825.png" alt="多输出通道"></p><h3 id="多输入多输出"><a href="#多输入多输出" class="headerlink" title="多输入多输出"></a>多输入多输出</h3><ul><li>每个通道可以识别特定模式</li><li>利用1*1卷积核,没有单通道的广度，可以多通道的融合，相当于全连接层</li></ul><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><ol><li>多输入通道互相关运算</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">corr2d_multi_in</span>(<span class="hljs-params">X,K</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(d2l.corr2d(x,k) <span class="hljs-keyword">for</span> x,k <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(X,K))<br></code></pre></td></tr></table></figure><ol start="2"><li>多输出通道互相关运算</li></ol><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">corr2d_multi_in_out</span>(X,K):<br>    return torch.<span class="hljs-built_in">stack</span>([<span class="hljs-built_in">corr2d_multi_in</span>(X,k) for k in K],<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>pytorch基础</title>
    <link href="/2025/08/17/25_08_17pytorch%E5%9F%BA%E7%A1%80/"/>
    <url>/2025/08/17/25_08_17pytorch%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="pytorch基础"><a href="#pytorch基础" class="headerlink" title="pytorch基础"></a>pytorch基础</h1><h2 id="自定义块"><a href="#自定义块" class="headerlink" title="自定义块"></a>自定义块</h2><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> torch<br><span class="hljs-title">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-title">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-class"></span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">MLP</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super().__init__()</span><br><span class="hljs-class">        self.hidden = nn.<span class="hljs-type">Linear</span>(20, 256)</span><br><span class="hljs-class">        self.out = nn.<span class="hljs-type">Linear</span>(256, 10)</span><br><span class="hljs-class"> </span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-type">X</span>):</span><br><span class="hljs-class">        return self.out(<span class="hljs-type">F</span>.<span class="hljs-title">relu</span>(<span class="hljs-title">self</span>.<span class="hljs-title">hidden</span>(<span class="hljs-type">X</span>)))</span><br><span class="hljs-class"></span><br></code></pre></td></tr></table></figure><h2 id="参数管理"><a href="#参数管理" class="headerlink" title="参数管理"></a>参数管理</h2><p>将神经网络（nn.Module）看作是一个参数容器，它包含了所有的参数（权重和偏置），以及一些方法（如前向传播）。</p><ol><li>参数访问<ul><li><code>net.state_dict()</code>：返回一个字典，包含了模型的所有参数（权重和偏置），还可以利用net[n]来提取第n层的参数</li><li><code>net[n].weight</code>：第n层的权重，<code>net[n].weight.data</code>：第n层的权重数据</li><li><code>net[n].bias</code>：第n层的偏置，<code>net[n].bias.data</code>：第n层的偏置数据</li><li><code>net.parameters()</code>：返回一个迭代器，包含了模型的所有参数</li></ul></li><li>参数初始化 <figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">init_normal</span>(m):<br>    if <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        nn.init.<span class="hljs-built_in">normal_</span>(m.weight, mean=<span class="hljs-number">0</span>, std=<span class="hljs-number">0.01</span>)<br>        nn.init.<span class="hljs-built_in">zeros_</span>(m.bias)<br><br>net.<span class="hljs-built_in">apply</span>(init_normal)#遍历所有层，对每个层应用init_normal函数<br></code></pre></td></tr></table></figure> <figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">init_constant</span>(m):<br>    if <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        nn.init.<span class="hljs-built_in">constant_</span>(m.weight, <span class="hljs-number">1</span>)<br>        nn.init.<span class="hljs-built_in">zeros_</span>(m.bias)<br><br>net.<span class="hljs-built_in">apply</span>(init_constant)<br></code></pre></td></tr></table></figure> 可以对不同层应用不同的初始化函数： <figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">init_xavier</span>(m):<br>    if <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        nn.init.<span class="hljs-built_in">xavier_uniform_</span>(m.weight)<br>        nn.init.<span class="hljs-built_in">zeros_</span>(m.bias)<br><br>def <span class="hljs-built_in">init_42</span>(m):<br>    if <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        nn.init.<span class="hljs-built_in">constant_</span>(m.weight, <span class="hljs-number">42</span>)<br>        nn.init.<span class="hljs-built_in">zeros_</span>(m.bias)<br>net[<span class="hljs-number">0</span>].<span class="hljs-built_in">apply</span>(init_xavier)<br>net[<span class="hljs-number">1</span>].<span class="hljs-built_in">apply</span>(init_42)<br></code></pre></td></tr></table></figure></li><li>参数绑定 <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">shared = nn<span class="hljs-selector-class">.Linear</span>(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>)<br>net = nn<span class="hljs-selector-class">.Sequential</span>(nn<span class="hljs-selector-class">.Linear</span>(<span class="hljs-number">4</span>, <span class="hljs-number">8</span>), nn<span class="hljs-selector-class">.ReLU</span>(),<br>                    shared, nn<span class="hljs-selector-class">.ReLU</span>(),<br>                    shared, nn<span class="hljs-selector-class">.ReLU</span>(),<br>                    nn<span class="hljs-selector-class">.Linear</span>(<span class="hljs-number">8</span>, <span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure> shared层的参数在不同层之间共享，即shared层的参数在不同层之间是指向同一个内存地址的</li></ol><h2 id="自定义层"><a href="#自定义层" class="headerlink" title="自定义层"></a>自定义层</h2><p>可以把层作为组件嵌套进更复杂的模型</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyLinear</span>(nn.<span class="hljs-title class_">Module</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):<br>        <span class="hljs-variable language_">super</span>().__init__()<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, X</span>):<br>        <span class="hljs-keyword">return</span> X - X.mean()<br></code></pre></td></tr></table></figure><p>带参数的层：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyLinear</span>(nn.<span class="hljs-title class_">Module</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, in_features, out_features</span>):<br>        <span class="hljs-variable language_">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.weight = nn.<span class="hljs-title class_">Parameter</span>(torch.randn(in_features, out_features))<br>        <span class="hljs-variable language_">self</span>.bias = nn.<span class="hljs-title class_">Parameter</span>(torch.zeros(out_features))<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, X</span>):<br>        <span class="hljs-keyword">return</span> X @ <span class="hljs-variable language_">self</span>.weight + <span class="hljs-variable language_">self</span>.bias<br></code></pre></td></tr></table></figure><p>自定义模型：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">MyMLP</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super().__init__()</span><br><span class="hljs-class">        self.linear = <span class="hljs-type">MyLinear</span>(20, 256)</span><br><span class="hljs-class">        self.out = nn.<span class="hljs-type">Linear</span>(256, 10)</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-type">X</span>):   </span><br><span class="hljs-class">        <span class="hljs-type">X</span> = self.linear(<span class="hljs-type">X</span>)  </span><br><span class="hljs-class">        return self.out(<span class="hljs-type">F</span>.<span class="hljs-title">relu</span>(<span class="hljs-type">X</span>))</span><br><span class="hljs-class">        # return self.out(<span class="hljs-type">X</span>)</span><br><span class="hljs-class">net = <span class="hljs-type">MyMLP</span>()</span><br></code></pre></td></tr></table></figure><h2 id="保存与加载"><a href="#保存与加载" class="headerlink" title="保存与加载"></a>保存与加载</h2><ol><li>加载和保存模型参数<ul><li><code>torch.save(net.state_dict(), &#39;net.params&#39;)</code>：保存模型参数</li><li><code>net.load_state_dict(torch.load(&#39;net.params&#39;))</code>：加载模型参数</li></ul></li><li>加载和保存整个模型<ul><li><code>torch.save(net, &#39;net.pt&#39;)</code>：保存整个模型</li><li><code>net = torch.load(&#39;net.pt&#39;)</code>：加载整个模型</li><li><code>net.eval()</code>：将模型设置为评估模式，即关闭dropout和batch normalization等层的训练模式</li><li><code>net.train()</code>：将模型设置为训练模式，即开启dropout和batch normalization等层的训练模式</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>LOAM代码笔记</title>
    <link href="/2025/08/16/25_08_16LOAM%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/08/16/25_08_16LOAM%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="LOAM代码笔记"><a href="#LOAM代码笔记" class="headerlink" title="LOAM代码笔记"></a>LOAM代码笔记</h1><h2 id="源码注解见本人的github"><a href="#源码注解见本人的github" class="headerlink" title="源码注解见本人的github"></a>源码注解见<a href="https://github.com/Fandy-Zhao/A-LOAM_note.git">本人的github</a></h2><h2 id="scanRegistration-cpp"><a href="#scanRegistration-cpp" class="headerlink" title="scanRegistration.cpp"></a><strong>scanRegistration.cpp</strong></h2><h3 id="目的："><a href="#目的：" class="headerlink" title="目的："></a>目的：</h3><p>实现激光点云的配准，主要包括点云的预处理、特征提取和配准过程。</p><h3 id="功能："><a href="#功能：" class="headerlink" title="功能："></a>功能：</h3><ol><li>订阅原始点云信息，对每周期点云按线序排列（原始信息是混乱的）。需要计算angle俯仰角和ori偏角，angle负责得出该点云的竖直方向的线序，ori负责的出点云的说平偏转度，两者组合成intensity。处理后的每个点云都有x,y,z,intensity四个信息。</li><li>对每周期点云进行特征提取，包括提取角点和面点。计算曲率cloudCurvature，根据曲率判断是否为角点或面点。</li><li>发布处理后的点云信息，包括角点和面点。<br><img src="/img/A_LOAM1.png" alt="流程整理"></li></ol><h2 id="laserOdometry-cpp"><a href="#laserOdometry-cpp" class="headerlink" title="laserOdometry.cpp"></a><strong>laserOdometry.cpp</strong></h2><h3 id="目的：-1"><a href="#目的：-1" class="headerlink" title="目的："></a>目的：</h3><p>通过读取scanRegistration.cpp中的信息来计算帧与帧之间的变化，最终得到里程计坐标系下的激光雷达的位姿。(前端激光里程计和位姿粗估计)</p><h3 id="功能：-1"><a href="#功能：-1" class="headerlink" title="功能："></a>功能：</h3><ol><li>订阅scanRegistration.cpp中的信息，包括角点和面点。<br>2.判断数据是否异常</li><li>将当前帧映射到上一帧，需要求解变换矩阵。<br><strong>偏原理的优化部分没太看懂，需要看一下论文再去找对应部分</strong></li><li>发布位姿（激光里程计）<br><img src="/img/A_LOAM2.png" alt="流程整理"></li></ol><h2 id="laserMapping-cpp"><a href="#laserMapping-cpp" class="headerlink" title="laserMapping.cpp"></a><strong>laserMapping.cpp</strong></h2><h3 id="目的：-2"><a href="#目的：-2" class="headerlink" title="目的："></a>目的：</h3><p>通过已经获得的激光里程计信息来消除激光里程计和地图之间的误差（也就是累计的误差），使得最终的姿态都是关于世界坐标的，并建图</p><h3 id="理解"><a href="#理解" class="headerlink" title="理解:"></a>理解:</h3><ol><li>先创建一个21<em>21</em>1的大空间，每次只关注自身周围的5<em>5</em>1的小空间。<br>2.可以通过大地图的动态调整，来使得小地图处于大地图的中心位置。</li><li>之后的数学计算不太理解，需要看一下论文再去找对应部分。</li></ol><p><img src="/img/A_LOAM3.png" alt="流程介绍"></p>]]></content>
    
    
    <categories>
      
      <category>SLAM</category>
      
      <category>LOAM</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>丢弃法  正则化</title>
    <link href="/2025/08/16/25_08_16%E4%B8%A2%E5%BC%83%E6%B3%95/"/>
    <url>/2025/08/16/25_08_16%E4%B8%A2%E5%BC%83%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a><strong>目的</strong></h2><p>在训练中加入偏差，使得训练更稳定</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><strong>训练</strong>时在隐藏层后加一层扰动，在加入噪音的基础上不改变原先的期望。在<strong>推理</strong>时不使用。</p><h2 id="内容讲解"><a href="#内容讲解" class="headerlink" title="内容讲解"></a>内容讲解</h2><p><img src="/img/dropout.png" alt="原理及方法介绍"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a><strong>代码</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">droppot_layer</span>(<span class="hljs-params">X,dropput</span>):<br>    <span class="hljs-keyword">assert</span> <span class="hljs-number">0</span> &lt;= dropput &lt;= <span class="hljs-number">1</span>   <span class="hljs-comment">#限制dropput的范围</span><br>    <span class="hljs-keyword">if</span> dropput == <span class="hljs-number">0</span>:  <span class="hljs-comment">#如果dropput为0,则</span><br>        <span class="hljs-keyword">return</span> X<br>    <span class="hljs-keyword">if</span> dropput == <span class="hljs-number">1</span>:  <span class="hljs-comment">#如果dropput为1,则</span><br>        <span class="hljs-keyword">return</span> torch.zeros_like(X)  <span class="hljs-comment">#返回与X同形状的全0张</span><br>    mask = (torch.randn(X.shape) &gt; dropput).<span class="hljs-built_in">float</span>()  <span class="hljs-comment">#生成与X同形状的mask张量（0或1 ）</span><br><br>    <span class="hljs-keyword">return</span> mask * X / (<span class="hljs-number">1.0</span> - dropput)  <span class="hljs-comment">#返回mask</span><br><br><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,num_inputs, num_outputs, num_hiddens1, num_hidden2, dropout1, dropout2</span>):<br>        <span class="hljs-built_in">super</span>(Net, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.flatten = nn.Flatten()<br>        <span class="hljs-variable language_">self</span>.linear1 = nn.Linear(num_inputs, num_hiddens1)       <span class="hljs-comment">#隐藏层1</span><br>        <span class="hljs-variable language_">self</span>.linear2 = nn.Linear(num_hiddens1, num_hidden2)      <span class="hljs-comment">#隐藏层2</span><br>        <span class="hljs-variable language_">self</span>.linear3 = nn.Linear(num_hidden2, num_outputs)       <span class="hljs-comment">#输出层</span><br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU()  <span class="hljs-comment"># 定义ReLU激活函数</span><br>        <span class="hljs-variable language_">self</span>.dropout1 = dropout1  <span class="hljs-comment">#隐藏层1的dropout率</span><br>        <span class="hljs-variable language_">self</span>.dropout2 = dropout2<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        H1 = torch.relu(<span class="hljs-variable language_">self</span>.linear1(<span class="hljs-variable language_">self</span>.flatten(X.reshape((-<span class="hljs-number">1</span>, num_inputs)))))  <span class="hljs-comment">#隐藏层1</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.training:  <span class="hljs-comment">#如果模型处于训练模式</span><br>            H1 = droppot_layer(H1, dropout1)  <span class="hljs-comment">#对隐藏层1应用dropout</span><br>        H2 = <span class="hljs-variable language_">self</span>.relu(<span class="hljs-variable language_">self</span>.linear2(H1))  <span class="hljs-comment">#隐藏层2</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.training:  <span class="hljs-comment">#如果模型处于训练模式</span><br>            H2 = droppot_layer(H2, dropout2)  <span class="hljs-comment">#对隐藏层2应用dropout</span><br>        out = <span class="hljs-variable language_">self</span>.linear3(H2)  <span class="hljs-comment">#输出层</span><br>        <span class="hljs-keyword">return</span> out<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    X = torch.arange(<span class="hljs-number">784</span>, dtype=torch.float32).reshape((<span class="hljs-number">28</span>, <span class="hljs-number">28</span>))<br>    num_inputs, num_outputs, num_hiddens1, num_hidden2 = <span class="hljs-number">784</span>,<span class="hljs-number">10</span>,<span class="hljs-number">256</span>,<span class="hljs-number">256</span><br>    dropout1, dropout2 = <span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span><br><br>    net = Net(num_inputs, num_outputs, num_hiddens1, num_hidden2, dropout1, dropout2)<br>    num_epochs, lr, batch_size = <span class="hljs-number">10</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">256</span><br>    loss = nn.CrossEntropyLoss()<br>    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)<br>    trainer = torch.optim.SGD(net.parameters(), lr=lr)<br>    d2l.train_ch3(<br>        net, train_iter, test_iter, loss, num_epochs, trainer<br>    )<br>    plt.show()  <span class="hljs-comment"># 确保脚本运行时弹出图像窗口</span><br>    <span class="hljs-built_in">print</span>(net(X))  <span class="hljs-comment"># 测试网络输出</span><br></code></pre></td></tr></table></figure><h2 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a><strong>个人理解</strong></h2><p>丢弃法是在训练时，随机舍弃几个输入值，并对其他输入做处理，即加入噪音又维持偏差为0</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>权重衰退  梯度爆炸</title>
    <link href="/2025/08/15/25_08_15%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80/"/>
    <url>/2025/08/15/25_08_15%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80/</url>
    
    <content type="html"><![CDATA[<h1 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a><strong>目的</strong></h2><p>限制权重w，防止梯度爆炸</p><h2 id="内容讲解"><a href="#内容讲解" class="headerlink" title="内容讲解"></a>内容讲解</h2><p><img src="/img/weight_decay.png" alt="原理及方法介绍"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a><strong>代码</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-comment">#生成人工数据集</span><br>n_train, n_test, num_inputs = <span class="hljs-number">20</span>, <span class="hljs-number">100</span>, <span class="hljs-number">200</span><br>batch_size = <span class="hljs-number">5</span><br>true_w, true_b = torch.ones((num_inputs, <span class="hljs-number">1</span>)) * <span class="hljs-number">0.01</span>, <span class="hljs-number">0.05</span><br>train_data = d2l.synthetic_data(true_w,true_b,n_train)<br>train_iter = d2l.load_array(train_data, batch_size)<br>test_data = d2l.synthetic_data(true_w,true_b,n_test)<br>test_iter = d2l.load_array(test_data, batch_size, is_train=<span class="hljs-literal">False</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_params</span>():<br>    w = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size=(num_inputs, <span class="hljs-number">1</span>), requires_grad=<span class="hljs-literal">True</span>)<br>    b = torch.zeros(<span class="hljs-number">1</span>, requires_grad=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> [w, b]<br><br><span class="hljs-comment"># L2范数惩罚(权重衰退的关键)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">l2_penalty</span>(<span class="hljs-params">w</span>):<br>    <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">sum</span>(w.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>)) / <span class="hljs-number">2</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">lambd</span>):<br>    w, b = init_params()<br>    net, loss = <span class="hljs-keyword">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss<br>    num_epochs, lr = <span class="hljs-number">100</span>, <span class="hljs-number">0.003</span><br><br>    <span class="hljs-comment">#训练,animator用于绘图</span><br>    animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epoch&#x27;</span>, ylabel=<span class="hljs-string">&#x27;loss&#x27;</span>, yscale=<span class="hljs-string">&#x27;log&#x27;</span>,<br>                            xlim=[<span class="hljs-number">5</span>, num_epochs], legend=[<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>])<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_iter:<br>            l = loss(net(X), y) + lambd * l2_penalty(w)<br>            l.<span class="hljs-built_in">sum</span>().backward()  <span class="hljs-comment">#计算梯度,backword()只能用于标量,所以要用sum()将l变成标量</span><br>            d2l.sgd([w, b], lr, batch_size)  <span class="hljs-comment">#使用小批量随机梯度下降迭代模型参数</span><br>        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:<br>            animator.add(epoch + <span class="hljs-number">1</span>, (d2l.evaluate_loss(net, train_iter, loss),<br>                                     d2l.evaluate_loss(net, test_iter, loss)))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;w的L2范数是：&#x27;</span>, torch.norm(w).item()) <br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment"># 训练时加权衰退</span><br>    train(lambd=<span class="hljs-number">10</span>)<br>    plt.show()  <br><br></code></pre></td></tr></table></figure><h2 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a><strong>个人理解</strong></h2><p>在训练时引用权重参数L2惩罚，可以有效限制权重w的增长，即总体损失和w的增长共同限制参数变化。因为当w变化过大时，随着深度增加，梯度呈指数级增长，会导致梯度爆炸。</p><h1 id="梯度爆炸-1"><a href="#梯度爆炸-1" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h1>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>深度学习知识树</title>
    <link href="/2025/08/14/25_08_14%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%A0%91/"/>
    <url>/2025/08/14/25_08_14%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<ol><li><p>感知机，多层感知机的实现</p></li><li><p>权重衰退  梯度爆炸</p></li><li><p>丢弃法  正则化</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>知识树</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>感知机</title>
    <link href="/2025/08/14/25_08_14%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <url>/2025/08/14/25_08_14%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<p>感知机是由输入input经过线性变化得到特定输出output(0或1)的结构<br>多层感知机引入<strong>非线性层&#x2F;隐藏层</strong>（激活函数）</p><p>下面是多层感知机手动实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><br>batch_size = <span class="hljs-number">256</span><br><span class="hljs-comment">#获取数据集</span><br>train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)<br><br>num_inputs = <span class="hljs-number">784</span>  <span class="hljs-comment">#28*28</span><br>num_outputs = <span class="hljs-number">10</span>  <span class="hljs-comment">#10个类别</span><br>num_hiddens = <span class="hljs-number">256</span>  <span class="hljs-comment">#隐藏层神经元个数(超参数)</span><br><br><span class="hljs-comment"># 训练层参数</span><br>W1 = nn.Parameter(<br>    torch.randn(num_inputs, num_hiddens, requires_grad=<span class="hljs-literal">True</span>)<span class="hljs-comment">#训练时反向传播需要梯度</span><br>)<br>b1 = nn.Parameter(<br>    torch.zeros(num_hiddens, requires_grad=<span class="hljs-literal">True</span>)<br>)<br><br>W2 = nn.Parameter(<br>    torch.randn(num_hiddens, num_outputs, requires_grad=<span class="hljs-literal">True</span>)<br>)<br>b2 = nn.Parameter(<br>    torch.zeros(num_outputs, requires_grad=<span class="hljs-literal">True</span>)<br>)<br><br>params = [W1, b1, W2, b2]<br><br><span class="hljs-comment">#实现激活函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">X</span>):<br>    a = torch.zeros_like(X)<br>    <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">max</span>(X, a)<br><br><span class="hljs-comment">#实现模型</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">net</span>(<span class="hljs-params">X</span>):<br>    X = X.reshape((-<span class="hljs-number">1</span>, num_inputs))  <span class="hljs-comment">#将输入展平</span><br>    H = relu(X @ W1 + b1)  <span class="hljs-comment">#隐藏层，@表示矩阵乘法</span><br>    <span class="hljs-keyword">return</span> H @ W2 + b2  <span class="hljs-comment">#输出层</span><br><br>num_epochs, lr = <span class="hljs-number">10</span>, <span class="hljs-number">0.1</span>  <span class="hljs-comment">#训练轮数，学习率</span><br>updater = torch.optim.SGD(params, lr=lr)  <span class="hljs-comment">#优化器</span><br>d2l.train_ch3(<br>    net, train_iter, test_iter, loss=nn.CrossEntropyLoss(),num_epochs=num_epochs,<br>    updater=updater<br>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>MIT Cheetah 3框架解读</title>
    <link href="/2025/08/14/25_08_14MIT-Cheetah-3%E6%A1%86%E6%9E%B6%E8%A7%A3%E8%AF%BB/"/>
    <url>/2025/08/14/25_08_14MIT-Cheetah-3%E6%A1%86%E6%9E%B6%E8%A7%A3%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>四足机器人</category>
      
      <category>控制框架</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>四足机器人运动学与动力学</title>
    <link href="/2025/08/12/25_08_12%E5%9B%9B%E8%B6%B3%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%BF%90%E5%8A%A8%E5%AD%A6%E4%B8%8E%E5%8A%A8%E5%8A%9B%E5%AD%A6/"/>
    <url>/2025/08/12/25_08_12%E5%9B%9B%E8%B6%B3%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%BF%90%E5%8A%A8%E5%AD%A6%E4%B8%8E%E5%8A%A8%E5%8A%9B%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="四足机器人运动学与动力学"><a href="#四足机器人运动学与动力学" class="headerlink" title="四足机器人运动学与动力学"></a>四足机器人运动学与动力学</h1><h2 id="运动学分析"><a href="#运动学分析" class="headerlink" title="运动学分析"></a>运动学分析</h2><h3 id="单腿运动学分析"><a href="#单腿运动学分析" class="headerlink" title="单腿运动学分析"></a>单腿运动学分析</h3><h4 id="正运动学分析"><a href="#正运动学分析" class="headerlink" title="正运动学分析"></a>正运动学分析</h4><p>概述：已知关节角度，求解足端位置<br>分类：三自由度串联腿，二自由度并联腿。（<strong>以三自由度串联腿为例，二自由度并联腿只需要去掉绕X轴的旋转关节,再加上一个电机角度的换算即可</strong>）<br>步骤：</p><ol><li>建立坐标系<br> 每个关节（自由度）都代表一个坐标系，以此实现腿部活动。如下图所示，可以建立三个坐标系，还有一个机体自身的坐标系<br> <img src="/img/leg_connect.png" alt="腿部连接示意图"></li><li>计算变换矩阵<br> 已知三个角度，每个关节对应一组旋转矩阵T(0,1),T(1,2),T(2,3)。<br> <strong>注意</strong>：左右腿的腿长互为相反数，便于表示</li><li>求解足端位置<br> 将旋转矩阵和基体系坐标累乘之后可以得到足端位置<br><strong>可以将三个坐标分别看作是三个角度组成的函数，引入雅各比矩阵<img src="/img/JJJ.png" alt="雅各比矩阵，对运动学正解求导"></strong></li></ol><h4 id="逆运动学"><a href="#逆运动学" class="headerlink" title="逆运动学"></a>逆运动学</h4><p>概述：已知足端位置，求解关节角度<br>步骤:<br><img src="/img/inv_single_1.png" alt="步骤一"><br><img src="/img/inv_single_2.png" alt="步骤二"><br><img src="/img/inv_single_3.png" alt="步骤三"></p><h3 id="单腿静力学分析"><a href="#单腿静力学分析" class="headerlink" title="单腿静力学分析"></a>单腿静力学分析</h3><p><strong>前提</strong>四足机器人静止，此时可以认为关节电机的功率和足端力的<strong>功率守恒</strong>（考虑做功守恒会有当前时刻做功不一样但总功一样，控制精度较差）<br><strong>公式</strong></p><ol><li>力矩乘以角加速度等于足端力乘以线速度</li><li>代入雅各比矩阵的公式化简</li><li>推出关节力矩和足端力的关系 <strong>（和运动学分析得到的雅各比矩阵有关）</strong><br><img src="/img/single_peace.png" alt="计算介绍"></li></ol><h3 id="四足运动学"><a href="#四足运动学" class="headerlink" title="四足运动学"></a>四足运动学</h3><h4 id="静止姿态的改变"><a href="#静止姿态的改变" class="headerlink" title="静止姿态的改变"></a>静止姿态的改变</h4><p><strong>目标</strong>：已知机身姿态，求关节角度<br>步骤：</p><ol><li>先假定右前足端为世界系的坐标原点，建立世界坐标系</li><li>计算四个足端相对于世界系的坐标P（s,i）&#x3D;T(s,b)P(b,i)</li><li>姿态不同，<strong>T</strong>会改变，P（b,i）已知，求解P（s,i）</li><li>由<strong>运动学逆解</strong>去推关节角度</li></ol><h4 id="足端速度"><a href="#足端速度" class="headerlink" title="足端速度"></a>足端速度</h4><p>足端速度分解：</p><ol><li>关节转动产生的速度v1(b)（用雅各比矩阵求）</li><li><strong>机身平移旋转</strong>产生的速度v2(b)（目标量）</li></ol><p>背景：机体有平移速度v0,Pi绕PO有角速度wi<br>v2(b)等于v0+wi*(p0-pi)<br>v(s)&#x3D;T(s,b)(v1(b)+v2(b))</p><h3 id="四足动力学"><a href="#四足动力学" class="headerlink" title="四足动力学"></a>四足动力学</h3>]]></content>
    
    
    <categories>
      
      <category>四足机器人</category>
      
      <category>运动学与动力学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>四足知识树整理</title>
    <link href="/2025/08/12/25_08_12%E5%9B%9B%E8%B6%B3%E7%9F%A5%E8%AF%86%E6%A0%91%E6%95%B4%E7%90%86/"/>
    <url>/2025/08/12/25_08_12%E5%9B%9B%E8%B6%B3%E7%9F%A5%E8%AF%86%E6%A0%91%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="四足知识树整理（整理中）"><a href="#四足知识树整理（整理中）" class="headerlink" title="四足知识树整理（整理中）"></a>四足知识树整理（整理中）</h1><h2 id="四足机器人大纲"><a href="#四足机器人大纲" class="headerlink" title="四足机器人大纲"></a>四足机器人大纲</h2><ol><li>控制方式：位控，<strong>力控</strong>，速控<br>作用：控制机器人静止或运动时的平衡状态</li><li>运动状态&#x2F;步态控制<br> 2.1 静态步态<br>2.1.1  <strong>trot步态(对角步态)</strong><br> 2.2 动态步态<br>2.2.1  跑步步态<br>2.2.2  飞跃步态<br> 2.3 步态规划</li><li>运动执行（和1. 控制方式相关）<br> 3.1 运动学分析<br>3.1.1  正运动学分析<br>3.1.2  逆运动学分析<br> 3.2 静力学分析<br> 3.3 动力学分析<br> 3.4 <strong>步态执行</strong></li><li>姿态估计<br> 4.1 IMU<br> 4.2卡尔曼滤波</li><li>位置获取<br> 5.1 激光雷达定位<br> 5.2 视觉定位<br> 5.3 IMU里程计定位<br> 5.4 融合定位</li><li>代码框架</li><li>路径规划<br> 6.1 比赛路径规划<br> 6.2 导航路径规划<br> 6.3 <strong>自动越野路径规划</strong></li></ol>]]></content>
    
    
    <categories>
      
      <category>四足机器人</category>
      
      <category>知识树</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/08/11/hello-world/"/>
    <url>/2025/08/11/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
  
  
  <entry>
    <title>about</title>
    <link href="/"/>
    <url>/</url>
    
    <content type="html"><![CDATA[<p>该博客用以记录自己的学习过程</p>]]></content>
    
  </entry>
  
  
  
</search>
